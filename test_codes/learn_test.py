import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from torch.cuda.amp import autocast, GradScaler
from torch.utils.data import DataLoader, random_split
from tqdm import tqdm
import time
import os
import re
from pathlib import Path
import json

# ===============================
# 1. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
# ===============================
def load_dataset(path, max_samples=None):
    """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã¿ã€è‹±èªã¨æ—¥æœ¬èªã®ãƒšã‚¢ã‚’è¿”ã™"""
    file_size = os.path.getsize(path)
    file_size_mb = file_size / (1024 * 1024)
    
    print(f"ğŸ“‚ File size: {file_size_mb:.2f} MB")
    print("ğŸ“– Reading dataset...")
    
    en, ja = [], []
    
    # è¡Œæ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
    with open(path, "r", encoding="utf-8") as f:
        total_lines = sum(1 for _ in f)
    
    # ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
    with open(path, "r", encoding="utf-8") as f:
        pbar = tqdm(f, total=total_lines, desc="Loading data", 
                    ncols=100, unit=" lines")
        for line in pbar:
            line = line.strip()
            if not line:
                continue
            try:
                # ã‚¿ãƒ–ã¾ãŸã¯è¤‡æ•°ã®ã‚¹ãƒšãƒ¼ã‚¹ã§åˆ†å‰²
                if "\t" in line:
                    parts = line.split("\t")
                else:
                    parts = re.split(r'\s{2,}', line)
                
                if len(parts) >= 2:
                    e = parts[0].strip()
                    j = parts[1].strip()
                    if e and j:
                        en.append(e)
                        ja.append(j)
                        
                        # max_samplesã«é”ã—ãŸã‚‰çµ‚äº†
                        if max_samples and len(en) >= max_samples:
                            break
            except Exception:
                continue
            
            if len(en) % 1000 == 0:
                pbar.set_postfix({"pairs": len(en)})
    
    return en, ja

# ===============================
# 2. Dataset ã‚¯ãƒ©ã‚¹ (ä¿®æ­£ç‰ˆ)
# ===============================
class TranslationDataset(torch.utils.data.Dataset):
    def __init__(self, en_list, ja_list, tokenizer, max_len=128):
        self.en = en_list
        self.ja = ja_list
        self.tok = tokenizer
        self.max_len = max_len
        
        # ãƒ¢ãƒ‡ãƒ«ãŒMarianMTã®å ´åˆã€ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’è¿½åŠ 
        self.add_prefix = hasattr(tokenizer, 'supported_language_codes')
    
    def __len__(self):
        return len(self.en)
    
    def __getitem__(self, idx):
        src = self.en[idx]
        tgt = self.ja[idx]
        
        # MarianMTãƒ¢ãƒ‡ãƒ«ã®å ´åˆã€ã‚½ãƒ¼ã‚¹è¨€èªã«ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’è¿½åŠ 
        if self.add_prefix:
            src = ">>jap<< " + src
        
        # ã‚½ãƒ¼ã‚¹ã®ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
        src_tok = self.tok(
            src, 
            max_length=self.max_len, 
            truncation=True,
            padding="max_length", 
            return_tensors="pt"
        )
        
        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®ãƒˆãƒ¼ã‚¯ãƒ³åŒ– (text_targetã‚’ä½¿ç”¨)
        tgt_tok = self.tok(
            text_target=tgt,  # text_targetãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
            max_length=self.max_len, 
            truncation=True,
            padding="max_length", 
            return_tensors="pt"
        )
        
        labels = tgt_tok["input_ids"].clone()
        labels[labels == self.tok.pad_token_id] = -100
        
        return {
            "input_ids": src_tok["input_ids"].squeeze(),
            "attention_mask": src_tok["attention_mask"].squeeze(),
            "labels": labels.squeeze(),
        }

# ===============================
# 3. æ¤œè¨¼é–¢æ•°
# ===============================
def evaluate_model(model, val_loader, device):
    """æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã§ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡"""
    model.eval()
    total_loss = 0
    
    with torch.no_grad():
        for batch in val_loader:
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            labels = batch["labels"].to(device)
            
            outputs = model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                labels=labels
            )
            total_loss += outputs.loss.item()
    
    return total_loss / len(val_loader)

# ===============================
# 4. Early Stopping
# ===============================
class EarlyStopping:
    def __init__(self, patience=3, min_delta=0.001):
        self.patience = patience
        self.min_delta = min_delta
        self.counter = 0
        self.best_loss = None
        self.early_stop = False
    
    def __call__(self, val_loss):
        if self.best_loss is None:
            self.best_loss = val_loss
        elif val_loss > self.best_loss - self.min_delta:
            self.counter += 1
            print(f"   âš ï¸ No improvement for {self.counter} epoch(s)")
            if self.counter >= self.patience:
                self.early_stop = True
        else:
            self.best_loss = val_loss
            self.counter = 0

# ===============================
# 5. å­¦ç¿’ã‚³ãƒ¼ãƒ‰
# ===============================
def train_model(
    model_name, 
    data_path, 
    epochs=10, 
    batch_size=16,  # ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å°ã•ã
    use_amp=True, 
    max_samples=50000,  # ã‚µãƒ³ãƒ—ãƒ«æ•°ã‚’æ¸›ã‚‰ã™
    val_split=0.1,
    save_dir="./models",
    learning_rate=5e-5,  # å­¦ç¿’ç‡ã‚’ä¸‹ã’ã‚‹
    gradient_clip=1.0,
    save_every=1,
    patience=3,  # Early stopping
    max_len=128  # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ã‚’é•·ã
):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print("=" * 60)
    print(f"ğŸš€ Using device: {device}")
    if torch.cuda.is_available():
        print(f"   GPU: {torch.cuda.get_device_name(0)}")
        print(f"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")
    if use_amp and device.type == "cuda":
        print("âš¡ Mixed Precision Training: ENABLED")
    print("=" * 60)
    
    # ãƒ¢ãƒ‡ãƒ«ã®èª­è¾¼
    print("\nğŸ“¦ Loading model and tokenizer...")
    start_time = time.time()
    tokenizer = AutoTokenizer.from_pretrained(model_name, use_safetensors=True)
    model = AutoModelForSeq2SeqLM.from_pretrained(model_name, use_safetensors=True).to(device)
    load_time = time.time() - start_time
    print(f"âœ… Model loaded in {load_time:.2f}s")
    print(f"   Model type: {model.config.model_type}")
    print(f"   Parameters: {sum(p.numel() for p in model.parameters()):,}")
    
    # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼æƒ…å ±
    print(f"\nğŸ“ Tokenizer info:")
    print(f"   Vocab size: {tokenizer.vocab_size}")
    if hasattr(tokenizer, 'supported_language_codes'):
        print(f"   Supported languages: {tokenizer.supported_language_codes}")
        print(f"   âš ï¸ Remember to add '>>jap<<' prefix to source text")
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    print(f"\nğŸ“š Loading dataset from {data_path}...")
    en_list, ja_list = load_dataset(data_path, max_samples=max_samples)
    print(f"âœ… Loaded {len(en_list):,} translation pairs")
    
    # ãƒ‡ãƒ¼ã‚¿ã®æ–¹å‘ã‚’ç¢ºèª
    print(f"\nğŸ” Checking data direction...")
    print(f"First 3 samples:")
    for i in range(min(3, len(en_list))):
        print(f"  EN: {en_list[i][:60]}...")
        print(f"  JA: {ja_list[i][:60]}...")
        print()
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
    full_dataset = TranslationDataset(en_list, ja_list, tokenizer, max_len=max_len)
    
    # è¨“ç·´/æ¤œè¨¼åˆ†å‰²
    val_size = int(len(full_dataset) * val_split)
    train_size = len(full_dataset) - val_size
    train_dataset, val_dataset = random_split(
        full_dataset, 
        [train_size, val_size],
        generator=torch.Generator().manual_seed(42)
    )
    
    print(f"\nğŸ“Š Dataset split:")
    print(f"   Training: {len(train_dataset):,} pairs")
    print(f"   Validation: {len(val_dataset):,} pairs")
    
    train_loader = DataLoader(
        train_dataset, 
        batch_size=batch_size, 
        shuffle=True, 
        num_workers=4,  # 2â†’4ã«å¢—ã‚„ã™
        pin_memory=True,
        prefetch_factor=2,  # äº‹å‰èª­ã¿è¾¼ã¿
        persistent_workers=True  # ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚’ç¶­æŒ
    )
    val_loader = DataLoader(
        val_dataset, 
        batch_size=batch_size * 2,  # æ¤œè¨¼æ™‚ã¯ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’2å€ã«
        shuffle=False, 
        num_workers=4,
        pin_memory=True,
        prefetch_factor=2,
        persistent_workers=True
    )
    
    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)
    
    # Gradient Accumulationç”¨ã®å¤‰æ•°
    accumulation_steps = 2  # 2ã‚¹ãƒ†ãƒƒãƒ—åˆ†ã®å‹¾é…ã‚’è“„ç©ã—ã¦ã‹ã‚‰æ›´æ–°
    
    # å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ© (å®Ÿéš›ã®ã‚¹ãƒ†ãƒƒãƒ—æ•°ã‚’èª¿æ•´)
    total_steps = (len(train_loader) // accumulation_steps) * epochs
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=total_steps, eta_min=1e-6)
    
    # Mixed Precisionç”¨ã®ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼
    scaler = GradScaler() if use_amp and device.type == "cuda" else None
    
    # Early Stopping
    early_stopping = EarlyStopping(patience=patience)
    
    # ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    Path(save_dir).mkdir(parents=True, exist_ok=True)
    
    # å­¦ç¿’ãƒ«ãƒ¼ãƒ—
    print(f"\nğŸ¯ Starting training")
    print(f"   Epochs: {epochs}")
    print(f"   Batch size: {batch_size}")
    print(f"   Learning rate: {learning_rate}")
    print(f"   Max sequence length: {max_len}")
    print(f"   Batches per epoch: {len(train_loader)}")
    print(f"   Early stopping patience: {patience}")
    print("=" * 60)
    
    total_start = time.time()
    history = {
        "epoch": [],
        "train_loss": [],
        "val_loss": [],
        "learning_rate": [],
        "time": []
    }
    
    best_val_loss = float('inf')
    
    for epoch in range(epochs):
        model.train()
        total_loss = 0
        epoch_start = time.time()
        
        pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{epochs}", 
                    ncols=120, bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]')
        
        for batch_idx, batch in enumerate(pbar):
            input_ids = batch["input_ids"].to(device, non_blocking=True)
            attention_mask = batch["attention_mask"].to(device, non_blocking=True)
            labels = batch["labels"].to(device, non_blocking=True)
            
            # Mixed Precision Training
            if scaler:
                with autocast():
                    outputs = model(
                        input_ids=input_ids,
                        attention_mask=attention_mask,
                        labels=labels
                    )
                    loss = outputs.loss / accumulation_steps  # å‹¾é…è“„ç©ã®ãŸã‚ã«lossã‚’å‰²ã‚‹
                
                scaler.scale(loss).backward()
                
                # å‹¾é…è“„ç©: accumulation_stepsã”ã¨ã«æ›´æ–°
                if (batch_idx + 1) % accumulation_steps == 0:
                    if gradient_clip > 0:
                        scaler.unscale_(optimizer)
                        torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clip)
                    
                    scaler.step(optimizer)
                    scaler.update()
                    scheduler.step()  # optimizerã®å¾Œã«å‘¼ã¶
                    optimizer.zero_grad()
            else:
                outputs = model(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    labels=labels
                )
                loss = outputs.loss / accumulation_steps
                loss.backward()
                
                # å‹¾é…è“„ç©: accumulation_stepsã”ã¨ã«æ›´æ–°
                if (batch_idx + 1) % accumulation_steps == 0:
                    if gradient_clip > 0:
                        torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clip)
                    
                    optimizer.step()
                    scheduler.step()  # optimizerã®å¾Œã«å‘¼ã¶
                    optimizer.zero_grad()
            
            total_loss += loss.item() * accumulation_steps  # è¡¨ç¤ºç”¨ã«æˆ»ã™
            
            # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã«ç¾åœ¨ã®lossã‚’è¡¨ç¤º
            pbar.set_postfix({
                "loss": f"{loss.item():.4f}", 
                "avg_loss": f"{total_loss/(batch_idx+1):.4f}",
                "lr": f"{scheduler.get_last_lr()[0]:.2e}"
            })
        
        # ã‚¨ãƒãƒƒã‚¯çµ‚äº†å¾Œã®å‡¦ç†
        epoch_time = time.time() - epoch_start
        avg_train_loss = total_loss / len(train_loader)
        
        # æ¤œè¨¼
        print(f"\nğŸ“Š Evaluating on validation set...")
        val_loss = evaluate_model(model, val_loader, device)
        
        # å±¥æ­´ã‚’ä¿å­˜
        current_lr = scheduler.get_last_lr()[0]
        history["epoch"].append(epoch + 1)
        history["train_loss"].append(avg_train_loss)
        history["val_loss"].append(val_loss)
        history["learning_rate"].append(current_lr)
        history["time"].append(epoch_time)
        
        # æå¤±ã®å·®ã‚’è¨ˆç®—
        loss_gap = abs(avg_train_loss - val_loss)
        
        print(f"\nğŸ“Š Epoch {epoch+1}/{epochs} Summary:")
        print(f"   Train Loss: {avg_train_loss:.4f}")
        print(f"   Val Loss: {val_loss:.4f}")
        print(f"   Gap: {loss_gap:.4f} {'âš ï¸ OVERFITTING!' if loss_gap > 0.15 else 'âœ…'}")
        print(f"   Learning Rate: {current_lr:.2e}")
        print(f"   Time: {epoch_time:.2f}s ({epoch_time/60:.2f}m)")
        print(f"   Avg time per batch: {epoch_time/len(train_loader):.3f}s")
        
        # ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ä¿å­˜
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            improvement = (best_val_loss - val_loss) / best_val_loss * 100 if epoch > 0 else 0
            print(f"   â­ New best validation loss! (improved by {improvement:.2f}%)")
            model.save_pretrained(os.path.join(save_dir, "best_model"), safe_serialization=True)
            tokenizer.save_pretrained(os.path.join(save_dir, "best_model"))
        
        # å®šæœŸä¿å­˜
        if (epoch + 1) % save_every == 0:
            checkpoint_dir = os.path.join(save_dir, f"checkpoint_epoch_{epoch+1}")
            model.save_pretrained(checkpoint_dir, safe_serialization=True)
            tokenizer.save_pretrained(checkpoint_dir)
            print(f"   ğŸ’¾ Checkpoint saved to {checkpoint_dir}")
        
        # Early Stopping ãƒã‚§ãƒƒã‚¯
        early_stopping(val_loss)
        if early_stopping.early_stop:
            print(f"\nğŸ›‘ Early stopping triggered after {epoch+1} epochs")
            break
        
        print("-" * 60)
    
    total_time = time.time() - total_start
    print("\n" + "=" * 60)
    print("ğŸ‰ Training completed!")
    print(f"   Total time: {total_time:.2f}s ({total_time/60:.2f}m)")
    print(f"   Average time per epoch: {total_time/len(history['epoch']):.2f}s")
    print(f"   Best validation loss: {best_val_loss:.4f}")
    print("=" * 60)
    
    # å­¦ç¿’å±¥æ­´ã®ä¿å­˜
    history_path = os.path.join(save_dir, "training_history.json")
    with open(history_path, "w") as f:
        json.dump(history, f, indent=2)
    print(f"\nğŸ’¾ Training history saved to {history_path}")
    
    # å­¦ç¿’å±¥æ­´ã®è¡¨ç¤º
    print("\nğŸ“ˆ Training History:")
    print(f"{'Epoch':<8} {'Train Loss':<12} {'Val Loss':<12} {'Gap':<10} {'LR':<12} {'Time':<10}")
    print("-" * 70)
    for ep, train_l, val_l, lr, t in zip(
        history["epoch"], 
        history["train_loss"], 
        history["val_loss"],
        history["learning_rate"],
        history["time"]
    ):
        gap = abs(train_l - val_l)
        print(f"{ep:<8} {train_l:<12.4f} {val_l:<12.4f} {gap:<10.4f} {lr:<12.2e} {t:<10.2f}s")
    
    # ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§è¿”ã™
    print(f"\nğŸ“¦ Loading best model...")
    model = AutoModelForSeq2SeqLM.from_pretrained(
        os.path.join(save_dir, "best_model"), 
        use_safetensors=True
    ).to(device)
    
    return model, tokenizer, history

# ===============================
# 6. ç¿»è¨³é–¢æ•° (ä¿®æ­£ç‰ˆ)
# ===============================
def translate(model, tokenizer, text, max_length=128, num_beams=5, device=None):
    """ãƒ†ã‚­ã‚¹ãƒˆã‚’ç¿»è¨³ (MarianMTå¯¾å¿œ)"""
    if device is None:
        device = next(model.parameters()).device
    
    model.eval()
    
    # MarianMTãƒ¢ãƒ‡ãƒ«ã®å ´åˆã€ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’è¿½åŠ 
    if hasattr(tokenizer, 'supported_language_codes'):
        text = ">>jap<< " + text
    
    # å…¥åŠ›ã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=max_length)
    inputs = {k: v.to(device) for k, v in inputs.items()}
    
    # ç”Ÿæˆ
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_length=max_length,
            num_beams=num_beams,
            early_stopping=True,
            no_repeat_ngram_size=3,
            length_penalty=1.0,
            repetition_penalty=1.2,
        )
    
    # ãƒ‡ã‚³ãƒ¼ãƒ‰
    translation = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return translation

# ===============================
# 7. ãƒãƒƒãƒç¿»è¨³
# ===============================
def batch_translate(model, tokenizer, texts, batch_size=8, max_length=128, num_beams=5):
    """è¤‡æ•°ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒãƒƒãƒã§ç¿»è¨³"""
    device = next(model.parameters()).device
    model.eval()
    
    # MarianMTãƒ¢ãƒ‡ãƒ«ã®å ´åˆã€ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’è¿½åŠ 
    if hasattr(tokenizer, 'supported_language_codes'):
        texts = [">>jap<< " + text for text in texts]
    
    translations = []
    
    for i in tqdm(range(0, len(texts), batch_size), desc="Translating"):
        batch_texts = texts[i:i+batch_size]
        
        inputs = tokenizer(batch_texts, return_tensors="pt", padding=True, truncation=True, max_length=max_length)
        inputs = {k: v.to(device) for k, v in inputs.items()}
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_length=max_length,
                num_beams=num_beams,
                early_stopping=True,
                no_repeat_ngram_size=3,
                length_penalty=1.0,
                repetition_penalty=1.2,
            )
        
        batch_translations = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]
        translations.extend(batch_translations)
    
    return translations

# ===============================
# å®Ÿè¡Œä¾‹
# ===============================
if __name__ == "__main__":
    # è¨­å®š
    MODEL = "Helsinki-NLP/opus-mt-en-jap"
    DATA = "./transformer/data/jesc/raw"
    SAVE_DIR = "./transformer/models/translation_model_v3"
    
    # å­¦ç¿’
    model, tokenizer, history = train_model(
        MODEL, 
        DATA, 
        epochs=3,  # ã‚¨ãƒãƒƒã‚¯æ•°ã‚’3ã«æ¸›ã‚‰ã™
        batch_size=32,  # ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’32ã«æˆ»ã™(64ã¯é…ã„)
        use_amp=True,
        max_samples=50000,  # 5ä¸‡ãƒšã‚¢ã«æ¸›ã‚‰ã™
        val_split=0.05,  # æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã‚’5%ã«æ¸›ã‚‰ã™(é«˜é€ŸåŒ–)
        save_dir=SAVE_DIR,
        learning_rate=1e-4,  # å­¦ç¿’ç‡ã‚’å°‘ã—ä¸Šã’ã‚‹
        gradient_clip=1.0,
        save_every=1,
        patience=2,  # Early stoppingã‚’2ã‚¨ãƒãƒƒã‚¯ã«
        max_len=64  # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ã‚’çŸ­ã(128â†’64)ã§é«˜é€ŸåŒ–
    )
    
    # ãƒ†ã‚¹ãƒˆ
    print("\n" + "=" * 60)
    print("ğŸ§ª Translation Test")
    print("=" * 60)
    
    test_sentences = [
        "I like apples.",
        "How are you?",
        "Good morning.",
        "This is a test sentence.",
        "Machine learning is fascinating.",
        "The weather is nice today.",
        "I am studying Japanese.",
        "Thank you very much."
    ]
    
    print("\nğŸ”¤ Single translations:")
    for sent in test_sentences:
        result = translate(model, tokenizer, sent)
        print(f"EN: {sent}")
        print(f"JA: {result}")
        print("-" * 60)
    
    print("\nğŸ”¤ Batch translations:")
    batch_results = batch_translate(model, tokenizer, test_sentences)
    for sent, result in zip(test_sentences, batch_results):
        print(f"EN: {sent}")
        print(f"JA: {result}")
        print("-" * 60)