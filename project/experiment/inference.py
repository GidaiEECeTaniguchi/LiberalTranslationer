import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import argparse
import sys
import time

def load_model(model_path, use_gpu=True):
    """ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’èª­ã¿è¾¼ã‚€"""
    print(f"ğŸ“‚ Loading model from: {model_path}")
    
    device = torch.device("cuda" if torch.cuda.is_available() and use_gpu else "cpu")
    print(f"ğŸ”§ Device: {device}")

    try:
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        model = AutoModelForSeq2SeqLM.from_pretrained(model_path)
        model.to(device)
        model.eval() # æ¨è«–ãƒ¢ãƒ¼ãƒ‰ã«å›ºå®š
        return model, tokenizer, device
    except Exception as e:
        print(f"âŒ Error loading model: {e}")
        sys.exit(1)

def translate(text, model, tokenizer, device, max_length=256, num_beams=5):
    """ç¿»è¨³ã‚’å®Ÿè¡Œã™ã‚‹é–¢æ•°"""
    
    # å­¦ç¿’æ™‚ã¨åŒã˜ã‚¿ã‚°ä»˜ã‘å‡¦ç†ï¼ˆé‡è¦ï¼‰
    # ã‚‚ã—å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã§å¸¸ã« ">>jap<< " ã‚’ã¤ã‘ã¦ã„ãŸãªã‚‰ã€ã“ã“ã§ã‚‚å¿…é ˆã§ã™
    # ä¸è¦ãªå ´åˆã¯ã“ã®è¡Œã‚’ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆã—ã¦ãã ã•ã„
    #input_text = f">>jap<< {text}" 
    input_text = text
    # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚º
    inputs = tokenizer(
        input_text, 
        return_tensors="pt", 
        padding=True, 
        truncation=True, 
        max_length=max_length
    ).to(device)

    # æ¨è«–ï¼ˆç”Ÿæˆï¼‰
    with torch.no_grad():
        start_time = time.time()
        outputs = model.generate(
            **inputs,
            max_length=max_length,
            num_beams=num_beams,
            min_length=5,             # æ¥µç«¯ã«çŸ­ã„å›ç­”ã‚’ç¦æ­¢
            no_repeat_ngram_size=3,   # ç¹°ã‚Šè¿”ã—ã‚’é˜²ãã€ç”Ÿæˆã‚’ä¿ƒã™
            early_stopping=False,
            do_sample=True,      # ğŸ‘ˆ æ±ºå®šè«–çš„ã§ã¯ãªãã€ç¢ºç‡çš„ã«é¸ã°ã›ã‚‹
            top_p=0.9,           # ğŸ‘ˆ ä¸Šä½90%ã®å€™è£œã‹ã‚‰é¸ã¶
            temperature=0.7,     # ğŸ‘ˆ å°‘ã—æŸ”ã‚‰ã‹ã„è¡¨ç¾ã‚’è¨±å¯ã™ã‚‹
        )
        end_time = time.time()

    # ãƒ‡ã‚³ãƒ¼ãƒ‰
    translated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    return translated_text, (end_time - start_time)

def main():
    parser = argparse.ArgumentParser(description="Custom Translation Inference")
    parser.add_argument("--model_dir", type=str, required=True, help="Path to the saved model directory")
    parser.add_argument("--text", type=str, help="Single text to translate")
    parser.add_argument("--interactive", action="store_true", help="Run in interactive mode")
    parser.add_argument("--cpu", action="store_true", help="Force CPU usage")
    
    args = parser.parse_args()

    # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
    model, tokenizer, device = load_model(args.model_dir, use_gpu=not args.cpu)

    # 1. å˜ç™ºå®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰
    if args.text:
        print(f"\nğŸ“¥ Input: {args.text}")
        result, latency = translate(args.text, model, tokenizer, device)
        print(f"ğŸ“¤ Output: {result}")
        print(f"â±ï¸ Latency: {latency:.4f} sec")

    # 2. å¯¾è©±ãƒ¢ãƒ¼ãƒ‰ï¼ˆãƒãƒ£ãƒƒãƒˆå½¢å¼ï¼‰
    elif args.interactive:
        print("\nğŸ’¬ Interactive Mode (Type 'exit' or 'q' to quit)")
        print("-" * 50)
        while True:
            try:
                user_input = input("EN > ")
                if user_input.lower() in ["exit", "q", "quit"]:
                    break
                if not user_input.strip():
                    continue
                
                result, latency = translate(user_input, model, tokenizer, device)
                print(f"JA > {result}")
                print(f"   (Time: {latency:.4f}s)") # é€Ÿåº¦ã‚’è¦‹ãŸã„å ´åˆã¯ã‚³ãƒ¡ãƒ³ãƒˆã‚¤ãƒ³
                print("-" * 20)
            except KeyboardInterrupt:
                break
        print("\nBye!")
    
    else:
        print("Please provide --text 'Your text' or use --interactive")

if __name__ == "__main__":
    main()
