import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from torch.cuda.amp import autocast, GradScaler
from torch.utils.data import DataLoader, random_split
from tqdm import tqdm
import random

import os
from pathlib import Path
import json

# ===============================
# 1. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ (JSONLå¯¾å¿œ)
# ===============================
def load_single_dataset(file_path, max_samples=None):
    """å˜ä¸€JSONLãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
    en_list, ja_list = [], []
    
    print(f"ğŸ“– Loading {file_path} ...")
    with open(file_path, "r", encoding="utf-8") as f:
        lines = f.readlines()
        
        # max_samplesãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        if max_samples and len(lines) > max_samples:
            print(f"  âš¡ Sampling {max_samples} from {len(lines)} lines")
            lines = random.sample(lines, max_samples)
        
        for line in tqdm(lines, desc=f"Reading {os.path.basename(file_path)}", unit=" lines"):
            try:
                data = json.loads(line)
                en, ja = data.get("en"), data.get("ja")
                if en and ja:
                    en_list.append(en)
                    ja_list.append(ja)
            except json.JSONDecodeError:
                continue
    
    print(f"  âœ… Loaded {len(en_list)} pairs from {os.path.basename(file_path)}")
    return en_list, ja_list


def load_datasets_balanced(file_paths, max_samples_per_type=None):
    """
    ByWorkç³»ã¨RandomSpanç³»ã‚’åˆ†ã‘ã¦ã€ãã‚Œãã‚Œã‹ã‚‰é©åˆ‡ã«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    
    Args:
        file_paths: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆ
        max_samples_per_type: RandomSpanç³»ã®å„ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å–å¾—ã™ã‚‹æœ€å¤§ã‚µãƒ³ãƒ—ãƒ«æ•°
                             (ByWorkç³»ã¯å…¨ã¦ä½¿ç”¨)
    
    Returns:
        bywork_files: [(file_path, en_list, ja_list), ...]
        span_files: [(file_path, en_list, ja_list), ...]
    """
    bywork_files = []
    span_files = []
    
    for fp in file_paths:
        is_bywork = "separated" in Path(fp).name or "sepalated" in Path(fp).name
        
        if is_bywork:
            # ByWorkç³»ã¯å…¨ã¦èª­ã¿è¾¼ã‚€
            print(f"\nğŸ¯ [WORK-LEVEL] {fp} (loading ALL)")
            en_list, ja_list = load_single_dataset(fp, max_samples=None)
            bywork_files.append((fp, en_list, ja_list))
        else:
            # RandomSpanç³»ã¯max_samples_per_typeåˆ†ã ã‘
            print(f"\nğŸ² [SPAN-LEVEL] {fp}")
            en_list, ja_list = load_single_dataset(fp, max_samples=max_samples_per_type)
            span_files.append((fp, en_list, ja_list))
    
    # ã‚µãƒãƒªãƒ¼è¡¨ç¤º
    print("\n" + "="*60)
    print("ğŸ“Š LOADING SUMMARY")
    print("="*60)
    
    total_bywork = sum(len(data[1]) for data in bywork_files)
    print(f"ByWork datasets: {len(bywork_files)} files, {total_bywork:,} pairs total")
    for fp, en_list, _ in bywork_files:
        print(f"  - {os.path.basename(fp)}: {len(en_list):,} pairs")
    
    total_span = sum(len(data[1]) for data in span_files)
    print(f"\nRandomSpan datasets: {len(span_files)} files, {total_span:,} pairs total")
    for fp, en_list, _ in span_files:
        print(f"  - {os.path.basename(fp)}: {len(en_list):,} pairs")
    
    print(f"\nğŸ‰ GRAND TOTAL: {total_bywork + total_span:,} pairs")
    print("="*60 + "\n")
    
    return bywork_files, span_files

# ===============================
# 2. Dataset ã‚¯ãƒ©ã‚¹
# ===============================
from torch.utils.data import Dataset

class TranslationDatasetRandomSpan(Dataset):
    def __init__(self, en_list, ja_list, tokenizer, max_len=128,
                 multi_prob=0.4,   # è¤‡æ•°æ–‡ã«ã™ã‚‹ç¢ºç‡
                 max_k=4):         # æœ€å¤§ä½•æ–‡ãã£ã¤ã‘ã‚‹ã‹
        self.en = en_list
        self.ja = ja_list
        self.tok = tokenizer
        self.max_len = max_len
        self.multi_prob = multi_prob
        self.max_k = max_k
        self.add_prefix = hasattr(tokenizer, 'supported_language_codes')

    def __len__(self):
        return len(self.en)

    def __getitem__(self, idx):
        L = len(self.en)

        # --- æ–‡å¯¾æ–‡ or è¤‡æ•°æ–‡ ---
        if random.random() < self.multi_prob:
            k = random.randint(1, self.max_k)
            left = max(0, idx - random.randint(0, k))
            right = min(L, idx + random.randint(1, k + 1))
        else:
            left, right = idx, idx + 1

        src = " ".join(self.en[left:right])
        tgt = " ".join(self.ja[left:right])

        if self.add_prefix:
            src = ">>jap<< " + src

        src_tok = self.tok(src, max_length=self.max_len, truncation=True,
                           padding="max_length", return_tensors="pt")
        tgt_tok = self.tok(text_target=tgt, max_length=self.max_len,
                           truncation=True, padding="max_length",
                           return_tensors="pt")

        labels = tgt_tok["input_ids"].clone()
        labels[labels == self.tok.pad_token_id] = -100

        return {
            "input_ids": src_tok["input_ids"].squeeze(),
            "attention_mask": src_tok["attention_mask"].squeeze(),
            "labels": labels.squeeze(),
        }


class TranslationDatasetByWork(torch.utils.data.Dataset):
    def __init__(self, en_list, ja_list, tokenizer, max_len=1024,
                 sep_en="%%%%%%%%THISWORKENDSHERE%%%%%%%%",
                 sep_ja="%%%%%%%%ã“ã®ä½œå“ã“ã“ã¾ã§%%%%%%%%"):
        self.tok = tokenizer
        self.max_len = max_len
        self.sep_en = sep_en
        self.sep_ja = sep_ja
        self.add_prefix = hasattr(tokenizer, 'supported_language_codes')

        # ---- ã“ã“ã§ä½œå“å˜ä½ã«ã¾ã¨ã‚ã‚‹ ----
        self.en_works = []
        self.ja_works = []

        cur_en = []
        cur_ja = []

        for en, ja in zip(en_list, ja_list):
            if en == self.sep_en and ja == self.sep_ja:
                # ä½œå“çµ‚äº† â†’ ãƒãƒƒãƒ•ã‚¡ã‚’å›ºã‚ã¦ä¿å­˜
                if cur_en and cur_ja:
                    self.en_works.append(" ".join(cur_en))
                    self.ja_works.append(" ".join(cur_ja))
                cur_en = []
                cur_ja = []
            else:
                cur_en.append(en)
                cur_ja.append(ja)

        # æœ€å¾Œã«ä½œå“ãŒçµ‚ã‚ã‚‰ãšæ®‹ã£ãŸå ´åˆ
        if cur_en and cur_ja:
            self.en_works.append(" ".join(cur_en))
            self.ja_works.append(" ".join(cur_ja))

    def __len__(self):
        return len(self.en_works)

    def __getitem__(self, idx):
        src = self.en_works[idx]
        tgt = self.ja_works[idx]

        # ç¿»è¨³å…ˆè¨€èªæŒ‡å®šãŒå¿…è¦ãªãƒ¢ãƒ‡ãƒ«ã®å ´åˆ(OPUSç³»ãªã©)
        if self.add_prefix:
            src = ">>jap<< " + src

        # ---- ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚º ----
        src_tok = self.tok(src, max_length=self.max_len, truncation=True,
                           padding="max_length", return_tensors="pt")

        tgt_tok = self.tok(text_target=tgt, max_length=self.max_len,
                           truncation=True, padding="max_length",
                           return_tensors="pt")

        labels = tgt_tok["input_ids"].clone()
        labels[labels == self.tok.pad_token_id] = -100

        return {
            "input_ids": src_tok["input_ids"].squeeze(),
            "attention_mask": src_tok["attention_mask"].squeeze(),
            "labels": labels.squeeze(),
        }


def build_combined_dataset(file_paths, tokenizer, max_len=256, 
                          max_samples_per_span_file=None):
    """
    ByWorkç³»ã¨RandomSpanç³»ã‚’é©åˆ‡ã«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã¦çµåˆ
    
    Args:
        file_paths: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆ
        tokenizer: ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼
        max_len: æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³é•·
        max_samples_per_span_file: RandomSpanç³»ã®å„ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å–ã‚‹æœ€å¤§ã‚µãƒ³ãƒ—ãƒ«æ•°
    """
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ (ãƒãƒ©ãƒ³ã‚¹èª¿æ•´æ¸ˆã¿)
    bywork_files, span_files = load_datasets_balanced(
        file_paths, 
        max_samples_per_type=max_samples_per_span_file
    )
    
    datasets = []
    
    # ByWorkç³»ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
    for fp, en_list, ja_list in bywork_files:
        ds = TranslationDatasetByWork(en_list, ja_list, tokenizer, max_len=max_len)
        datasets.append(ds)
        print(f"âœ… Created ByWork dataset from {os.path.basename(fp)}: {len(ds)} works")
    
    # RandomSpanç³»ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
    for fp, en_list, ja_list in span_files:
        ds = TranslationDatasetRandomSpan(en_list, ja_list, tokenizer, max_len=max_len)
        datasets.append(ds)
        print(f"âœ… Created RandomSpan dataset from {os.path.basename(fp)}: {len(ds)} pairs")
    
    # è¤‡æ•° dataset ã‚’é€£çµ
    from torch.utils.data import ConcatDataset
    combined = ConcatDataset(datasets)
    print(f"\nğŸ¯ Combined dataset total size: {len(combined)}")
    
    return combined

# ===============================
# 3. æ¤œè¨¼é–¢æ•°
# ===============================
def evaluate_model(model, val_loader, device):
    model.eval()
    total_loss = 0
    with torch.no_grad():
        for batch in val_loader:
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            labels = batch["labels"].to(device)
            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
            total_loss += outputs.loss.item()
    return total_loss / len(val_loader)

# ===============================
# 4. Early Stopping
# ===============================
class EarlyStopping:
    def __init__(self, patience=3, min_delta=0.001):
        self.patience = patience
        self.min_delta = min_delta
        self.counter = 0
        self.best_loss = None
        self.early_stop = False
    
    def __call__(self, val_loss):
        if self.best_loss is None:
            self.best_loss = val_loss
        elif val_loss > self.best_loss - self.min_delta:
            self.counter += 1
            print(f"âš ï¸ No improvement for {self.counter} epoch(s)")
            if self.counter >= self.patience:
                self.early_stop = True
        else:
            self.best_loss = val_loss
            self.counter = 0

# ===============================
# 5. å­¦ç¿’é–¢æ•°
# ===============================
def train_model(
    model_name,
    file_paths,
    epochs=3,
    batch_size=32,
    use_amp=True,
    max_samples_per_span_file=None,  # RandomSpanç³»ã®å„ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã®æœ€å¤§ã‚µãƒ³ãƒ—ãƒ«æ•°
    val_split=0.05,
    save_dir="./models",
    learning_rate=1e-4,
    gradient_clip=1.0,
    save_every=1,
    patience=2,
    max_len=64
):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸš€ Using device: {device}")
    
    tokenizer = AutoTokenizer.from_pretrained(model_name, use_safetensors=True)
    model = AutoModelForSeq2SeqLM.from_pretrained(model_name, use_safetensors=True).to(device)
    
    # æ”¹å–„ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ§‹ç¯‰
    dataset = build_combined_dataset(
        file_paths, 
        tokenizer, 
        max_len=max_len,
        max_samples_per_span_file=max_samples_per_span_file
    )
    
    val_size = int(len(dataset) * val_split)
    train_size = len(dataset) - val_size
    train_dataset, val_dataset = random_split(
        dataset, [train_size, val_size], 
        generator=torch.Generator().manual_seed(42)
    )
    
    print(f"\nğŸ“Š Dataset split:")
    print(f"  Training: {train_size:,} samples")
    print(f"  Validation: {val_size:,} samples\n")
    
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size*2, shuffle=False)
    
    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)
    scaler = GradScaler() if use_amp and device.type == "cuda" else None
    early_stopping = EarlyStopping(patience=patience)
    
    best_val_loss = float('inf')
    os.makedirs(save_dir, exist_ok=True)
    
    for epoch in range(epochs):
        model.train()
        total_loss = 0
        pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{epochs}")
        for batch in pbar:
            optimizer.zero_grad()
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            labels = batch["labels"].to(device)
            
            if scaler:
                with autocast():
                    outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
                    loss = outputs.loss
                scaler.scale(loss).backward()
                if gradient_clip > 0:
                    scaler.unscale_(optimizer)
                    torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clip)
                scaler.step(optimizer)
                scaler.update()
            else:
                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
                loss = outputs.loss
                loss.backward()
                if gradient_clip > 0:
                    torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clip)
                optimizer.step()
            
            total_loss += loss.item()
            pbar.set_postfix({"loss": f"{loss.item():.4f}"})
        
        val_loss = evaluate_model(model, val_loader, device)
        print(f"ğŸ“Š Validation loss: {val_loss:.4f}")
        
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            model.save_pretrained(os.path.join(save_dir, "best_model"))
            tokenizer.save_pretrained(os.path.join(save_dir, "best_model"))
            print("â­ New best model saved!")
        
        early_stopping(val_loss)
        if early_stopping.early_stop:
            print(f"ğŸ›‘ Early stopping triggered at epoch {epoch+1}")
            break
    
    return model, tokenizer

# ===============================
# 6. ç¿»è¨³é–¢æ•°
# ===============================
def translate(model, tokenizer, text, max_length=64, num_beams=4):
    if hasattr(tokenizer, 'supported_language_codes'):
        text = ">>jap<< " + text
    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=max_length)
    device = next(model.parameters()).device
    inputs = {k: v.to(device) for k, v in inputs.items()}
    with torch.no_grad():
        outputs = model.generate(**inputs, max_length=max_length, num_beams=num_beams)
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

def batch_translate(model, tokenizer, texts, batch_size=8, max_length=64, num_beams=4):
    device = next(model.parameters()).device
    if hasattr(tokenizer, 'supported_language_codes'):
        texts = [">>jap<< " + t for t in texts]
    translations = []
    for i in range(0, len(texts), batch_size):
        batch_texts = texts[i:i+batch_size]
        inputs = tokenizer(batch_texts, return_tensors="pt", padding=True, truncation=True, max_length=max_length)
        inputs = {k: v.to(device) for k, v in inputs.items()}
        with torch.no_grad():
            outputs = model.generate(**inputs, max_length=max_length, num_beams=num_beams)
        translations.extend([tokenizer.decode(o, skip_special_tokens=True) for o in outputs])
    return translations

# ===============================
# å®Ÿè¡Œä¾‹
# ===============================
if __name__ == "__main__":
    files = [
        "./../data/sepalated_dataset.jsonl",           # ByWorkç³»
        "./../data/OpenSubtitles_sample_40000.jsonl",  # RandomSpanç³»
        "./../data/TED_sample_40000.jsonl",            # RandomSpanç³»
        "./../data/Tatoeba_sample_40000.jsonl",        # RandomSpanç³»
        "./../data/all_outenjp.jsonl"                  # RandomSpanç³» 
    ]
   
    MODEL_NAME = "Helsinki-NLP/opus-mt-en-jap"
    SAVE_DIR = "./models/translation_model_balanced"
    
    model, tokenizer = train_model(
        MODEL_NAME,
        files,
        epochs=2,
        batch_size=16,
        max_samples_per_span_file=40000,  # RandomSpanç³»ã¯å„ãƒ•ã‚¡ã‚¤ãƒ«40000ä»¶ã¾ã§
        save_dir=SAVE_DIR
    )
    
    test_sentences = ["I like apples.", "How are you?", "Machine learning is fun."]
    results = batch_translate(model, tokenizer, test_sentences)
    for en, ja in zip(test_sentences, results):
        print(f"EN: {en} -> JA: {ja}")