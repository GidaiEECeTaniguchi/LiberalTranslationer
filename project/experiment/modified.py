import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, get_linear_schedule_with_warmup, DataCollatorForSeq2Seq
from torch.cuda.amp import autocast, GradScaler
from torch.utils.data import DataLoader, random_split
from torch.optim.lr_scheduler import OneCycleLR
from tqdm import tqdm
import random
from torch.utils.data import Dataset, Subset
import os
from pathlib import Path
import json
import logging
from dataclasses import dataclass, field
from typing import List, Optional

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


# ===============================
# 0. è¨­å®šã‚¯ãƒ©ã‚¹
# ===============================
@dataclass
class TrainingConfig:
    model_name: str
    file_paths: List[str]
    epochs: int = 3
    batch_size: int = 32
    use_amp: bool = True
    max_samples_per_span_file: Optional[int] = None
    val_split: float = 0.05
    save_dir: str = "./models"
    learning_rate: float = 1e-4
    gradient_clip: float = 1.0
    patience: int = 2
    max_len: int = 64
    random_seed: int = 42
    tags: Optional[List[str]] = None
    num_workers: int = 4
    accumulation_steps: int = 4
    use_bfloat16: bool = True
    use_compile: bool = False  # torch.compileã¯äº’æ›æ€§å•é¡ŒãŒã‚ã‚‹ãŸã‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆOFF
    scheduler_type: str = 'onecycle'
    warmup_steps: int = 500


# ===============================
# 1. ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãªãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
# ===============================
def load_single_dataset_streaming(file_path, max_samples=None, random_seed=42, tag=None):
    """
    ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ã«JSONLãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
    å…¨è¡Œã‚’ä¸€åº¦ã«ãƒ¡ãƒ¢ãƒªã«èª­ã¿è¾¼ã¾ãšã€å¿…è¦ãªåˆ†ã ã‘å‡¦ç†
    
    Args:
        file_path: èª­ã¿è¾¼ã‚€ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        max_samples: æœ€å¤§ã‚µãƒ³ãƒ—ãƒ«æ•°
        random_seed: ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰
        tag: è‹±æ–‡ã®å…ˆé ­ã«è¿½åŠ ã™ã‚‹ã‚¿ã‚° (ä¾‹: "[LYRICS]")
    """
    en_list, ja_list = [], []
    error_count = 0
    
    logger.info(f"ğŸ“– Loading {file_path} ...")
    
    with open(file_path, "r", encoding="utf-8") as f:
        # max_samplesãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆ
        if max_samples:
            # ã¾ãšå…¨è¡Œæ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ(ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ã«)
            total_lines = sum(1 for _ in f)
            f.seek(0)  # ãƒ•ã‚¡ã‚¤ãƒ«ãƒã‚¤ãƒ³ã‚¿ã‚’å…ˆé ­ã«æˆ»ã™
            
            if total_lines > max_samples:
                logger.info(f"  âš¡ Sampling {max_samples} from {total_lines} lines")
                # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã™ã‚‹è¡Œç•ªå·ã‚’äº‹å‰ã«æ±ºå®š
                random.seed(random_seed)
                selected_indices = set(random.sample(range(total_lines), max_samples))
                
                # é¸æŠã•ã‚ŒãŸè¡Œã ã‘ã‚’å‡¦ç†
                for idx, line in enumerate(tqdm(f, total=total_lines,
                                                desc=f"Reading {os.path.basename(file_path)}",
                                                unit=" lines")):
                    if idx in selected_indices:
                        try:
                            data = json.loads(line)
                            en, ja = data.get("en"), data.get("ja")
                            if en and ja and len(en.strip()) > 0 and len(ja.strip()) > 0:
                                # ğŸ†• ã‚¿ã‚°ã‚’è¿½åŠ 
                                if tag:
                                    en = f"{tag} {en}"
                                en_list.append(en)
                                ja_list.append(ja)
                        except json.JSONDecodeError:
                            error_count += 1
            else:
                # å…¨è¡Œã‚’å‡¦ç†
                f.seek(0)
                for line in tqdm(f, total=total_lines,
                               desc=f"Reading {os.path.basename(file_path)}",
                               unit=" lines"):
                    try:
                        data = json.loads(line)
                        en, ja = data.get("en"), data.get("ja")
                        if en and ja and len(en.strip()) > 0 and len(ja.strip()) > 0:
                            # ğŸ†• ã‚¿ã‚°ã‚’è¿½åŠ 
                            if tag:
                                en = f"{tag} {en}"
                            en_list.append(en)
                            ja_list.append(ja)
                    except json.JSONDecodeError:
                        error_count += 1
        else:
            # max_samplesãªã—ã®å ´åˆã¯å…¨è¡Œã‚’å‡¦ç†
            for line in tqdm(f, desc=f"Reading {os.path.basename(file_path)}", unit=" lines"):
                try:
                    data = json.loads(line)
                    en, ja = data.get("en"), data.get("ja")
                    if en and ja and len(en.strip()) > 0 and len(ja.strip()) > 0:
                        # ğŸ†• ã‚¿ã‚°ã‚’è¿½åŠ 
                        if tag:
                            en = f"{tag} {en}"
                        en_list.append(en)
                        ja_list.append(ja)
                except json.JSONDecodeError:
                    error_count += 1
    
    if error_count > 0:
        logger.warning(f"  âš ï¸  Skipped {error_count} invalid lines")
    
    logger.info(f"  âœ… Loaded {len(en_list)} pairs from {os.path.basename(file_path)}")
    return en_list, ja_list


def load_datasets_balanced(file_paths, max_samples_per_type=None, random_seed=42, tags=None):
    """
    ByWorkç³»ã¨RandomSpanç³»ã‚’åˆ†ã‘ã¦ã€ãã‚Œãã‚Œã‹ã‚‰é©åˆ‡ã«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    
    Args:
        file_paths: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆ
        max_samples_per_type: RandomSpanç³»ã®å„ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å–å¾—ã™ã‚‹æœ€å¤§ã‚µãƒ³ãƒ—ãƒ«æ•°
        random_seed: ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰
        tags: å„ãƒ•ã‚¡ã‚¤ãƒ«ã«å¯¾å¿œã™ã‚‹ã‚¿ã‚°ã®ãƒªã‚¹ãƒˆ (Noneã®å ´åˆã¯ã‚¿ã‚°ãªã—)
    
    Returns:
        bywork_files: [(file_path, en_list, ja_list), ...]
        span_files: [(file_path, en_list, ja_list), ...]
    """
    bywork_files = []
    span_files = []
    
    # tagsãŒNoneã®å ´åˆã¯å…¨ã¦Noneã®ãƒªã‚¹ãƒˆã‚’ä½œæˆ
    if tags is None:
        tags = [None] * len(file_paths)
    
    for fp, tag in zip(file_paths, tags):
        is_bywork = "separated" in Path(fp).name or "sepalated" in Path(fp).name
        
        if is_bywork:
            # ByWorkç³»ã¯å…¨ã¦èª­ã¿è¾¼ã‚€
            logger.info(f"\nğŸ¯ [WORK-LEVEL] {fp} (loading ALL)")
            en_list, ja_list = load_single_dataset_streaming(fp, max_samples=None, random_seed=random_seed, tag=tag)
            bywork_files.append((fp, en_list, ja_list))
        else:
            # RandomSpanç³»ã¯max_samples_per_typeåˆ†ã ã‘
            logger.info(f"\nğŸ² [SPAN-LEVEL] {fp}")
            en_list, ja_list = load_single_dataset_streaming(fp, max_samples=max_samples_per_type, random_seed=random_seed, tag=tag)
            span_files.append((fp, en_list, ja_list))
    
    # ã‚µãƒãƒªãƒ¼è¡¨ç¤º
    logger.info("\n" + "="*60)
    logger.info("ğŸ“Š LOADING SUMMARY")
    logger.info("="*60)
    
    total_bywork = sum(len(data[1]) for data in bywork_files)
    logger.info(f"ByWork datasets: {len(bywork_files)} files, {total_bywork:,} pairs total")
    for fp, en_list, _ in bywork_files:
        logger.info(f"  - {os.path.basename(fp)}: {len(en_list):,} pairs")
    
    total_span = sum(len(data[1]) for data in span_files)
    logger.info(f"\nRandomSpan datasets: {len(span_files)} files, {total_span:,} pairs total")
    for fp, en_list, _ in span_files:
        logger.info(f"  - {os.path.basename(fp)}: {len(en_list):,} pairs")
    
    logger.info(f"\nğŸ‰ GRAND TOTAL: {total_bywork + total_span:,} pairs")
    logger.info("="*60 + "\n")
    
    return bywork_files, span_files

# ===============================
# 2. ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãª Dataset ã‚¯ãƒ©ã‚¹
# ===============================


# RandomSpan ç”¨ collatorï¼ˆdynamic padding + label smoothingï¼‰
def build_randomspan_collator(tokenizer, label_smoothing=0.1):
    return DataCollatorForSeq2Seq(
        tokenizer=tokenizer,
        model=None,  # loss ã¯ model å´ã§
        padding="longest",
        label_pad_token_id=-100,
        pad_to_multiple_of=8  # TensorCore åŠ¹ç‡
    )


def split_concat_dataset(dataset):
    bywork_indices = []
    span_indices = []

    for i, ds in enumerate(dataset.datasets):
        if isinstance(ds, TranslationDatasetByWorkMemoryEfficient):
            bywork_indices.extend(range(dataset.cumulative_sizes[i - 1] if i > 0 else 0,
                                         dataset.cumulative_sizes[i]))
        else:
            span_indices.extend(range(dataset.cumulative_sizes[i - 1] if i > 0 else 0,
                                       dataset.cumulative_sizes[i]))

    return bywork_indices, span_indices


class TranslationDatasetRandomSpan(Dataset):
    """ãƒ©ãƒ³ãƒ€ãƒ ã‚¹ãƒ‘ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ"""

    def __init__(self, en_list, ja_list, tokenizer, max_len=128,
                 multi_prob=0.4,  # è¤‡æ•°æ–‡ã«ã™ã‚‹ç¢ºç‡
                 max_k=4):  # æœ€å¤§ä½•æ–‡ãã£ã¤ã‘ã‚‹ã‹
        self.en = en_list
        self.ja = ja_list
        self.tok = tokenizer
        self.max_len = max_len
        self.multi_prob = multi_prob
        self.max_k = max_k
        self.add_prefix = hasattr(tokenizer, 'supported_language_codes')

    def __len__(self):
        return len(self.en)

    def __getitem__(self, idx):
        L = len(self.en)

        # --- æ–‡å¯¾æ–‡ or è¤‡æ•°æ–‡ ---
        if random.random() < self.multi_prob:
            k = random.randint(1, self.max_k)
            left = max(0, idx - random.randint(0, k))
            right = min(L, idx + random.randint(1, k + 1))
        else:
            left, right = idx, idx + 1

        src = " ".join(self.en[left:right])
        tgt = " ".join(self.ja[left:right])

        if self.add_prefix:
            src = ">>jap<< " + src

        src_tok = self.tok(src, max_length=self.max_len, truncation=True,
                           padding="max_length", return_tensors="pt")
        tgt_tok = self.tok(text_target=tgt, max_length=self.max_len,
                           truncation=True, padding="max_length",
                           return_tensors="pt")

        labels = tgt_tok["input_ids"].clone()
        labels[labels == self.tok.pad_token_id] = -100

        return {
            "input_ids": src_tok["input_ids"].squeeze(),
            "attention_mask": src_tok["attention_mask"].squeeze(),
            "labels": labels.squeeze(),
        }

    def set_multi_prob(self, value: float):
        self.multi_prob = max(0.0, min(1.0, value))


class TranslationDatasetByWorkMemoryEfficient(torch.utils.data.Dataset):
    """
    ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãªByWorkãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
    
    æ”¹å–„ç‚¹:
    1. __init__ã§å…¨ä½œå“ã‚’ãƒ¡ãƒ¢ãƒªã«å±•é–‹ã›ãšã€ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¨ä½ç½®æƒ…å ±ã®ã¿ä¿æŒ
    2. __getitem__ã§å¿…è¦ãªæ™‚ã«å¿…è¦ãªä½œå“ã ã‘ã‚’æ§‹ç¯‰
    """

    def __init__(self, en_list, ja_list, tokenizer, max_len=1024,
                 sep_en="%%%%%%%%THISWORKENDSHERE%%%%%%%%",
                 sep_ja="%%%%%%%%ã“ã®ä½œå“ã“ã“ã¾ã§%%%%%%%%"):
        self.en_list = en_list  # å…ƒã®ãƒªã‚¹ãƒˆã¸ã®å‚ç…§ã‚’ä¿æŒ
        self.ja_list = ja_list
        self.tok = tokenizer
        self.max_len = max_len
        self.sep_en = sep_en
        self.sep_ja = sep_ja
        self.add_prefix = hasattr(tokenizer, 'supported_language_codes')

        # ä½œå“ã®å¢ƒç•Œã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ã¿ã‚’ä¿å­˜(ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„)
        self.work_boundaries = []  # [(start_idx, end_idx), ...]
        
        start_idx = 0
        for i, (en, ja) in enumerate(zip(en_list, ja_list)):
            if en == self.sep_en and ja == self.sep_ja:
                if i > start_idx:  # ç©ºã®ä½œå“ã‚’é¿ã‘ã‚‹
                    self.work_boundaries.append((start_idx, i))
                start_idx = i + 1
        
        # æœ€å¾Œã®ä½œå“
        if start_idx < len(en_list):
            self.work_boundaries.append((start_idx, len(en_list)))
        
        logger.info(f"  ğŸ“š Found {len(self.work_boundaries)} works in ByWork dataset")

    def __len__(self):
        return len(self.work_boundaries)

    def __getitem__(self, idx):
        # å¿…è¦ãªä½œå“ã®ã¿ã‚’ãã®å ´ã§æ§‹ç¯‰
        start_idx, end_idx = self.work_boundaries[idx]
        
        src = " ".join(self.en_list[start_idx:end_idx])
        tgt = " ".join(self.ja_list[start_idx:end_idx])

        # ç¿»è¨³å…ˆè¨€èªæŒ‡å®šãŒå¿…è¦ãªãƒ¢ãƒ‡ãƒ«ã®å ´åˆ(OPUSç³»ãªã©)
        if self.add_prefix:
            src = ">>jap<< " + src

        # ---- ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚º ----
        src_tok = self.tok(src, max_length=self.max_len, truncation=True,
                           padding="max_length", return_tensors="pt")

        tgt_tok = self.tok(text_target=tgt, max_length=self.max_len,
                           truncation=True, padding="max_length",
                           return_tensors="pt")

        labels = tgt_tok["input_ids"].clone()
        labels[labels == self.tok.pad_token_id] = -100

        return {
            "input_ids": src_tok["input_ids"].squeeze(),
            "attention_mask": src_tok["attention_mask"].squeeze(),
            "labels": labels.squeeze(),
        }


def build_combined_dataset(file_paths, tokenizer, max_len=256,
                          max_samples_per_span_file=None, random_seed=42, tags=None):
    """
    ByWorkç³»ã¨RandomSpanç³»ã‚’é©åˆ‡ã«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã¦çµåˆ
    
    Args:
        file_paths: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆ
        tokenizer: ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼
        max_len: æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³é•·
        max_samples_per_span_file: RandomSpanç³»ã®å„ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å–ã‚‹æœ€å¤§ã‚µãƒ³ãƒ—ãƒ«æ•°
        random_seed: ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰
        tags: å„ãƒ•ã‚¡ã‚¤ãƒ«ã«å¯¾å¿œã™ã‚‹ã‚¿ã‚°ã®ãƒªã‚¹ãƒˆ
    """
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ (ãƒãƒ©ãƒ³ã‚¹èª¿æ•´æ¸ˆã¿)
    bywork_files, span_files = load_datasets_balanced(
        file_paths,
        max_samples_per_type=max_samples_per_span_file,
        random_seed=random_seed,
        tags=tags
    )
    
    datasets = []
    
    # ByWorkç³»ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ(ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãƒãƒ¼ã‚¸ãƒ§ãƒ³ä½¿ç”¨)
    for fp, en_list, ja_list in bywork_files:
        ds = TranslationDatasetByWorkMemoryEfficient(en_list, ja_list, tokenizer, max_len=max_len)
        datasets.append(ds)
        logger.info(f"âœ… Created ByWork dataset from {os.path.basename(fp)}: {len(ds)} works")
    
    # RandomSpanç³»ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
    for fp, en_list, ja_list in span_files:
        ds = TranslationDatasetRandomSpan(en_list, ja_list, tokenizer, max_len=max_len)
        datasets.append(ds)
        logger.info(f"âœ… Created RandomSpan dataset from {os.path.basename(fp)}: {len(ds)} pairs")
    
    # è¤‡æ•° dataset ã‚’é€£çµ
    from torch.utils.data import ConcatDataset
    combined = ConcatDataset(datasets)
    logger.info(f"\nğŸ¯ Combined dataset total size: {len(combined)}")
    
    return combined


# ===============================
# 3. æ¤œè¨¼é–¢æ•°
# ===============================
def evaluate_model(model, val_loader, device):
    model.eval()
    total_loss = 0
    with torch.no_grad():
        for batch in val_loader:
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            labels = batch["labels"].to(device)
            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
            total_loss += outputs.loss.item()
    return total_loss / len(val_loader)


# ===============================
# 4. Early Stopping
# ===============================
class EarlyStopping:

    def __init__(self, patience=3, min_delta=0.001):
        self.patience = patience
        self.min_delta = min_delta
        self.counter = 0
        self.best_loss = None
        self.early_stop = False
    
    def __call__(self, val_loss):
        if self.best_loss is None:
            self.best_loss = val_loss
        elif val_loss > self.best_loss - self.min_delta:
            self.counter += 1
            logger.info(f"âš ï¸ No improvement for {self.counter} epoch(s)")
            if self.counter >= self.patience:
                self.early_stop = True
        else:
            self.best_loss = val_loss
            self.counter = 0


def freeze_encoder_layers(model, ratio=0.5):
    enc_layers = model.model.encoder.layers
    freeze_until = int(len(enc_layers) * ratio)
    for i, layer in enumerate(enc_layers):
        for p in layer.parameters():
            p.requires_grad = i >= freeze_until


# ===============================
# 5. ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°ã•ã‚ŒãŸå­¦ç¿’é–¢æ•°ç¾¤
# ===============================

def setup_training(config: TrainingConfig):
    """ãƒ‡ãƒã‚¤ã‚¹ã€ãƒ­ã‚®ãƒ³ã‚°ã€ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã€ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–"""
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    logger.info(f"ğŸš€ Using device: {device}")
    
    logger.info("\n" + "="*60 + "\nâš¡ SPEED OPTIMIZATION SETTINGS\n" + "="*60)
    logger.info(f"âœ“ DataLoader workers: {config.num_workers}")
    logger.info(f"âœ“ Gradient accumulation steps: {config.accumulation_steps}")
    logger.info(f"âœ“ Effective batch size: {config.batch_size * config.accumulation_steps}")
    logger.info(f"âœ“ BFloat16: {config.use_bfloat16 and str(device) == 'cuda'}")
    logger.info(f"âœ“ torch.compile: {config.use_compile}")
    logger.info(f"âœ“ Scheduler type: {config.scheduler_type}")
    if config.scheduler_type == 'linear_warmup': logger.info(f"âœ“ Warmup steps: {config.warmup_steps}")
    logger.info("="*60 + "\n")
    
    tokenizer = AutoTokenizer.from_pretrained(config.model_name, use_safetensors=True)
    model = AutoModelForSeq2SeqLM.from_pretrained(config.model_name, use_safetensors=True).to(device)
    model.config.dropout = 0.15
    model.config.attention_dropout = 0.15
    
    if config.use_compile and hasattr(torch, 'compile'):
        try:
            import torch._dynamo
            torch._dynamo.config.suppress_errors = True
            model = torch.compile(model, mode='reduce-overhead')
            logger.info("âœ… Model compiled successfully!")
        except Exception as e:
            logger.warning(f"âš ï¸ torch.compile failed: {e}. Continuing without compilation.")
            
    return device, tokenizer, model

def create_dataloaders(config: TrainingConfig, tokenizer):
    """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã®æ§‹ç¯‰"""
    dataset = build_combined_dataset(
        config.file_paths,
        tokenizer,
        max_len=config.max_len,
        max_samples_per_span_file=config.max_samples_per_span_file,
        random_seed=config.random_seed,
        tags=config.tags
    )

    # Split dataset into training and validation sets
    val_size = int(len(dataset) * config.val_split)
    train_size = len(dataset) - val_size
    train_dataset, val_dataset = random_split(
        dataset, [train_size, val_size],
        generator=torch.Generator().manual_seed(config.random_seed)
    )

    # Separate indices for bywork and span datasets within the training set
    bywork_idx, span_idx = split_concat_dataset(dataset)
    train_indices = set(train_dataset.indices)
    train_bywork_idx = [i for i in bywork_idx if i in train_indices]
    train_span_idx = [i for i in span_idx if i in train_indices]

    train_bywork = Subset(dataset, train_bywork_idx)
    train_span = Subset(dataset, train_span_idx)
    
    logger.info(f"\nğŸ“Š Dataset split:")
    logger.info(f"  Training: {len(train_dataset):,} samples ({len(train_bywork)} by-work, {len(train_span)} span)")
    logger.info(f"  Validation: {len(val_dataset):,} samples\n")

    span_collator = build_randomspan_collator(tokenizer)

    loader_args = {'num_workers': config.num_workers, 'pin_memory': True, 'persistent_workers': config.num_workers > 0}

    train_loader_span = DataLoader(train_span, batch_size=config.batch_size, shuffle=True, collate_fn=span_collator, **loader_args)
    train_loader_bywork = DataLoader(train_bywork, batch_size=max(1, config.batch_size // 4), shuffle=True, **loader_args)
    val_loader = DataLoader(val_dataset, batch_size=config.batch_size * 2, shuffle=False, **loader_args)
    
    return [train_loader_span, train_loader_bywork], val_loader, dataset


def create_optimizer_and_scheduler(model, config: TrainingConfig, train_loaders):
    """ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã¨ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã®ä½œæˆ"""
    optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)
    
    total_steps_per_epoch = sum(len(loader) for loader in train_loaders)
    total_steps = total_steps_per_epoch * config.epochs // config.accumulation_steps

    if config.scheduler_type == 'onecycle':
        scheduler = OneCycleLR(optimizer, max_lr=config.learning_rate * 10, total_steps=total_steps, pct_start=0.3, anneal_strategy='cos', div_factor=25.0, final_div_factor=1e4)
        logger.info(f"ğŸ“ˆ OneCycleLR scheduler initialized (total_steps={total_steps})")
    elif config.scheduler_type == 'linear_warmup':
        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=config.warmup_steps, num_training_steps=total_steps)
        logger.info(f"ğŸ“ˆ Linear warmup scheduler initialized (warmup_steps={config.warmup_steps}, total_steps={total_steps})")
    else:
        scheduler = None
        
    return optimizer, scheduler

def train_epoch(model, loaders, optimizer, scheduler, scaler, device, config: TrainingConfig, epoch: int):
    """1ã‚¨ãƒãƒƒã‚¯åˆ†ã®å­¦ç¿’å‡¦ç†"""
    model.train()
    total_loss = 0
    use_bf16 = config.use_bfloat16 and device.type == "cuda" and torch.cuda.is_bf16_supported()

    for loader in loaders:
        pbar = tqdm(loader, desc=f"Epoch {epoch + 1}/{config.epochs}")
        for batch_idx, batch in enumerate(pbar):
            if batch_idx % config.accumulation_steps == 0:
                optimizer.zero_grad(set_to_none=True)

            input_ids, attention_mask, labels = batch["input_ids"].to(device), batch["attention_mask"].to(device), batch["labels"].to(device)
            
            loss_divisor = config.accumulation_steps
            autocast_args = {'dtype': torch.bfloat16} if use_bf16 else {}
            
            with autocast(**autocast_args):
                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
                loss = outputs.loss / loss_divisor

            if scaler:
                scaler.scale(loss).backward()
            else:
                loss.backward()

            if (batch_idx + 1) % config.accumulation_steps == 0:
                if config.gradient_clip > 0:
                    if scaler: scaler.unscale_(optimizer)
                    torch.nn.utils.clip_grad_norm_(model.parameters(), config.gradient_clip)
                
                if scaler:
                    scaler.step(optimizer)
                    scaler.update()
                else:
                    optimizer.step()
                
                if scheduler: scheduler.step()

            batch_loss = loss.item() * loss_divisor
            total_loss += batch_loss
            pbar.set_postfix(loss=f"{batch_loss:.4f}", lr=f"{optimizer.param_groups[0]['lr']:.2e}")
            
    return total_loss

def train_model(config: TrainingConfig):
    """æœ€é©åŒ–ã•ã‚ŒãŸå­¦ç¿’é–¢æ•°ï¼ˆãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°ç‰ˆï¼‰"""
    device, tokenizer, model = setup_training(config)
    train_loaders, val_loader, dataset = create_dataloaders(config, tokenizer)
    optimizer, scheduler = create_optimizer_and_scheduler(model, config, train_loaders)
    
    use_bf16 = config.use_bfloat16 and device.type == "cuda" and torch.cuda.is_bf16_supported()
    if config.use_bfloat16 and not use_bf16: logger.warning("âš ï¸ BFloat16 requested but not supported. Falling back to FP16.")
    scaler = GradScaler() if config.use_amp and device.type == "cuda" and not use_bf16 else None
    
    early_stopping = EarlyStopping(patience=config.patience)
    best_val_loss = float('inf')
    os.makedirs(config.save_dir, exist_ok=True)
    
    freeze_encoder_layers(model, ratio=0.5)
    logger.info("ğŸ”’ Encoder layers partially frozen (ratio=0.5)")

    for epoch in range(config.epochs):
        start_prob, end_prob = 0.5, 0.1
        current_prob = start_prob + (end_prob - start_prob) * (epoch / max(1, config.epochs - 1))
        for ds in dataset.datasets:
            if isinstance(ds, TranslationDatasetRandomSpan): ds.set_multi_prob(current_prob)
        logger.info(f"ğŸ“‰ RandomSpan multi_prob = {current_prob:.2f}")

        train_loss = train_epoch(model, train_loaders, optimizer, scheduler, scaler, device, config, epoch)
        
        if epoch == 1:
            for p in model.parameters(): p.requires_grad = True
            logger.info("ğŸ”“ Encoder fully unfrozen")

        val_loss = evaluate_model(model, val_loader, device)
        total_train_samples = sum(len(l.dataset) for l in train_loaders)
        avg_train_loss = train_loss / total_train_samples if total_train_samples > 0 else 0
        logger.info(f"ğŸ“Š Epoch {epoch+1}/{config.epochs} -> Train loss: {avg_train_loss:.4f}, Validation loss: {val_loss:.4f}")
        
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            save_path = os.path.join(config.save_dir, "best_model")
            model.save_pretrained(save_path)
            tokenizer.save_pretrained(save_path)
            logger.info(f"â­ New best model saved to {save_path}!")
        
        early_stopping(val_loss)
        if early_stopping.early_stop:
            logger.info(f"ğŸ›‘ Early stopping triggered at epoch {epoch+1}")
            break
    
    logger.info(f"\nâœ… Training completed! Best validation loss: {best_val_loss:.4f}")
    return model, tokenizer

# ===============================
# 6. ç¿»è¨³é–¢æ•°
# ===============================
def translate(model, tokenizer, text, max_length=64, num_beams=4):
    if hasattr(tokenizer, 'supported_language_codes'):
        text = ">>jap<< " + text
    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=max_length)
    device = next(model.parameters()).device
    inputs = {k: v.to(device) for k, v in inputs.items()}
    with torch.no_grad():
        outputs = model.generate(**inputs, max_length=max_length, num_beams=num_beams)
    return tokenizer.decode(outputs[0], skip_special_tokens=True)


def batch_translate(model, tokenizer, texts, batch_size=8, max_length=64, num_beams=4):
    device = next(model.parameters()).device
    if hasattr(tokenizer, 'supported_language_codes'):
        texts = [">>jap<< " + t for t in texts]
    translations = []
    for i in range(0, len(texts), batch_size):
        batch_texts = texts[i:i + batch_size]
        inputs = tokenizer(batch_texts, return_tensors="pt", padding=True, truncation=True, max_length=max_length)
        inputs = {k: v.to(device) for k, v in inputs.items()}
        with torch.no_grad():
            outputs = model.generate(**inputs, max_length=max_length, num_beams=num_beams)
        translations.extend([tokenizer.decode(o, skip_special_tokens=True) for o in outputs])
    return translations


# ===============================
# å®Ÿè¡Œä¾‹
# ===============================
if __name__ == "__main__":
    files = [
        "./../data/sepalated_dataset.jsonl",  # ByWorkç³»
        "./../data/OpenSubtitles_sample_40000.jsonl",  # RandomSpanç³»
        "./../data/TED_sample_40000.jsonl",  # RandomSpanç³»
        "./../data/Tatoeba_sample_40000.jsonl",  # RandomSpanç³»
        "./../data/all_outenjp.jsonl"  # RandomSpanç³» (æ­Œè©ãªã©)
    ]
    
    # ğŸ†• å„ãƒ•ã‚¡ã‚¤ãƒ«ã«å¯¾å¿œã™ã‚‹ã‚¿ã‚° (å¿…è¦ã«å¿œã˜ã¦è¨­å®š)
    # tags = [None, None, None, None, "[LYRICS]"]  # æ­Œè©ãƒ‡ãƒ¼ã‚¿ã«ã‚¿ã‚°ã‚’ä»˜ã‘ã‚‹ä¾‹
    tags = None  # ã‚¿ã‚°ãªã—ã®å ´åˆ
   
    # --- å­¦ç¿’è¨­å®š ---
    config = TrainingConfig(
        model_name="Helsinki-NLP/opus-mt-en-jap",
        file_paths=files,
        epochs=2,
        batch_size=16,
        max_samples_per_span_file=40000,
        save_dir="./models/translation_model_final",
        random_seed=42,
        tags=tags,
        # --- é«˜é€ŸåŒ–è¨­å®š ---
        num_workers=4,
        accumulation_steps=4,
        use_bfloat16=True,
        scheduler_type='onecycle'
    )
    
    # --- å­¦ç¿’å®Ÿè¡Œ ---
    model, tokenizer = train_model(config)
    
    # --- ç¿»è¨³ãƒ†ã‚¹ãƒˆ ---
    test_sentences = [
        "I like apples.",
        "How are you?",
        "Machine learning is fun.",
        "I couldn't speak English well."
    ]
    results = batch_translate(model, tokenizer, test_sentences)
    for en, ja in zip(test_sentences, results):
        print(f"EN: {en} -> JA: {ja}")
