import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, get_linear_schedule_with_warmup, DataCollatorForSeq2Seq
from torch.cuda.amp import autocast, GradScaler
from torch.utils.data import DataLoader, random_split
from torch.optim.lr_scheduler import OneCycleLR
from tqdm import tqdm
import random
from torch.utils.data import Dataset, Subset
import os
from pathlib import Path
import json
import logging

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


# ===============================
# 1. ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãªãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
# ===============================
def load_single_dataset_streaming(file_path, max_samples=None, random_seed=42, tag=None):
    """
    ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ã«JSONLãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
    å…¨è¡Œã‚’ä¸€åº¦ã«ãƒ¡ãƒ¢ãƒªã«èª­ã¿è¾¼ã¾ãšã€å¿…è¦ãªåˆ†ã ã‘å‡¦ç†
    
    Args:
        file_path: èª­ã¿è¾¼ã‚€ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        max_samples: æœ€å¤§ã‚µãƒ³ãƒ—ãƒ«æ•°
        random_seed: ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰
        tag: è‹±æ–‡ã®å…ˆé ­ã«è¿½åŠ ã™ã‚‹ã‚¿ã‚° (ä¾‹: "[LYRICS]")
    """
    en_list, ja_list = [], []
    error_count = 0
    
    logger.info(f"ğŸ“– Loading {file_path} ...")
    
    with open(file_path, "r", encoding="utf-8") as f:
        # max_samplesãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆ
        if max_samples:
            # ã¾ãšå…¨è¡Œæ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ(ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ã«)
            total_lines = sum(1 for _ in f)
            f.seek(0)  # ãƒ•ã‚¡ã‚¤ãƒ«ãƒã‚¤ãƒ³ã‚¿ã‚’å…ˆé ­ã«æˆ»ã™
            
            if total_lines > max_samples:
                logger.info(f"  âš¡ Sampling {max_samples} from {total_lines} lines")
                # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã™ã‚‹è¡Œç•ªå·ã‚’äº‹å‰ã«æ±ºå®š
                random.seed(random_seed)
                selected_indices = set(random.sample(range(total_lines), max_samples))
                
                # é¸æŠã•ã‚ŒãŸè¡Œã ã‘ã‚’å‡¦ç†
                for idx, line in enumerate(tqdm(f, total=total_lines,
                                                desc=f"Reading {os.path.basename(file_path)}",
                                                unit=" lines")):
                    if idx in selected_indices:
                        try:
                            data = json.loads(line)
                            en, ja = data.get("en"), data.get("ja")
                            if en and ja and len(en.strip()) > 0 and len(ja.strip()) > 0:
                                # ğŸ†• ã‚¿ã‚°ã‚’è¿½åŠ 
                                if tag:
                                    en = f"{tag} {en}"
                                en_list.append(en)
                                ja_list.append(ja)
                        except json.JSONDecodeError:
                            error_count += 1
            else:
                # å…¨è¡Œã‚’å‡¦ç†
                f.seek(0)
                for line in tqdm(f, total=total_lines,
                               desc=f"Reading {os.path.basename(file_path)}",
                               unit=" lines"):
                    try:
                        data = json.loads(line)
                        en, ja = data.get("en"), data.get("ja")
                        if en and ja and len(en.strip()) > 0 and len(ja.strip()) > 0:
                            # ğŸ†• ã‚¿ã‚°ã‚’è¿½åŠ 
                            if tag:
                                en = f"{tag} {en}"
                            en_list.append(en)
                            ja_list.append(ja)
                    except json.JSONDecodeError:
                        error_count += 1
        else:
            # max_samplesãªã—ã®å ´åˆã¯å…¨è¡Œã‚’å‡¦ç†
            for line in tqdm(f, desc=f"Reading {os.path.basename(file_path)}", unit=" lines"):
                try:
                    data = json.loads(line)
                    en, ja = data.get("en"), data.get("ja")
                    if en and ja and len(en.strip()) > 0 and len(ja.strip()) > 0:
                        # ğŸ†• ã‚¿ã‚°ã‚’è¿½åŠ 
                        if tag:
                            en = f"{tag} {en}"
                        en_list.append(en)
                        ja_list.append(ja)
                except json.JSONDecodeError:
                    error_count += 1
    
    if error_count > 0:
        logger.warning(f"  âš ï¸  Skipped {error_count} invalid lines")
    
    logger.info(f"  âœ… Loaded {len(en_list)} pairs from {os.path.basename(file_path)}")
    return en_list, ja_list


def load_datasets_balanced(file_paths, max_samples_per_type=None, random_seed=42, tags=None):
    """
    ByWorkç³»ã¨RandomSpanç³»ã‚’åˆ†ã‘ã¦ã€ãã‚Œãã‚Œã‹ã‚‰é©åˆ‡ã«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    
    Args:
        file_paths: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆ
        max_samples_per_type: RandomSpanç³»ã®å„ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å–å¾—ã™ã‚‹æœ€å¤§ã‚µãƒ³ãƒ—ãƒ«æ•°
        random_seed: ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰
        tags: å„ãƒ•ã‚¡ã‚¤ãƒ«ã«å¯¾å¿œã™ã‚‹ã‚¿ã‚°ã®ãƒªã‚¹ãƒˆ (Noneã®å ´åˆã¯ã‚¿ã‚°ãªã—)
    
    Returns:
        bywork_files: [(file_path, en_list, ja_list), ...]
        span_files: [(file_path, en_list, ja_list), ...]
    """
    bywork_files = []
    span_files = []
    
    # tagsãŒNoneã®å ´åˆã¯å…¨ã¦Noneã®ãƒªã‚¹ãƒˆã‚’ä½œæˆ
    if tags is None:
        tags = [None] * len(file_paths)
    
    for fp, tag in zip(file_paths, tags):
        is_bywork = "separated" in Path(fp).name or "sepalated" in Path(fp).name
        
        if is_bywork:
            # ByWorkç³»ã¯å…¨ã¦èª­ã¿è¾¼ã‚€
            logger.info(f"\nğŸ¯ [WORK-LEVEL] {fp} (loading ALL)")
            en_list, ja_list = load_single_dataset_streaming(fp, max_samples=None, random_seed=random_seed, tag=tag)
            bywork_files.append((fp, en_list, ja_list))
        else:
            # RandomSpanç³»ã¯max_samples_per_typeåˆ†ã ã‘
            logger.info(f"\nğŸ² [SPAN-LEVEL] {fp}")
            en_list, ja_list = load_single_dataset_streaming(fp, max_samples=max_samples_per_type, random_seed=random_seed, tag=tag)
            span_files.append((fp, en_list, ja_list))
    
    # ã‚µãƒãƒªãƒ¼è¡¨ç¤º
    logger.info("\n" + "="*60)
    logger.info("ğŸ“Š LOADING SUMMARY")
    logger.info("="*60)
    
    total_bywork = sum(len(data[1]) for data in bywork_files)
    logger.info(f"ByWork datasets: {len(bywork_files)} files, {total_bywork:,} pairs total")
    for fp, en_list, _ in bywork_files:
        logger.info(f"  - {os.path.basename(fp)}: {len(en_list):,} pairs")
    
    total_span = sum(len(data[1]) for data in span_files)
    logger.info(f"\nRandomSpan datasets: {len(span_files)} files, {total_span:,} pairs total")
    for fp, en_list, _ in span_files:
        logger.info(f"  - {os.path.basename(fp)}: {len(en_list):,} pairs")
    
    logger.info(f"\nğŸ‰ GRAND TOTAL: {total_bywork + total_span:,} pairs")
    logger.info("="*60 + "\n")
    
    return bywork_files, span_files

# ===============================
# 2. ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãª Dataset ã‚¯ãƒ©ã‚¹
# ===============================


# RandomSpan ç”¨ collatorï¼ˆdynamic padding + label smoothingï¼‰
def build_randomspan_collator(tokenizer, label_smoothing=0.1):
    return DataCollatorForSeq2Seq(
        tokenizer=tokenizer,
        model=None,  # loss ã¯ model å´ã§
        padding="longest",
        label_pad_token_id=-100,
        pad_to_multiple_of=8  # TensorCore åŠ¹ç‡
    )


def split_concat_dataset(dataset):
    bywork_indices = []
    span_indices = []

    for i, ds in enumerate(dataset.datasets):
        if isinstance(ds, TranslationDatasetByWorkMemoryEfficient):
            bywork_indices.extend(range(dataset.cumulative_sizes[i - 1] if i > 0 else 0,
                                         dataset.cumulative_sizes[i]))
        else:
            span_indices.extend(range(dataset.cumulative_sizes[i - 1] if i > 0 else 0,
                                       dataset.cumulative_sizes[i]))

    return bywork_indices, span_indices


class TranslationDatasetRandomSpan(Dataset):
    """ãƒ©ãƒ³ãƒ€ãƒ ã‚¹ãƒ‘ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ"""

    def __init__(self, en_list, ja_list, tokenizer, max_len=128,
                 multi_prob=0.4,  # è¤‡æ•°æ–‡ã«ã™ã‚‹ç¢ºç‡
                 max_k=4):  # æœ€å¤§ä½•æ–‡ãã£ã¤ã‘ã‚‹ã‹
        self.en = en_list
        self.ja = ja_list
        self.tok = tokenizer
        self.max_len = max_len
        self.multi_prob = multi_prob
        self.max_k = max_k
        self.add_prefix = hasattr(tokenizer, 'supported_language_codes')

    def __len__(self):
        return len(self.en)

    def __getitem__(self, idx):
        L = len(self.en)

        # --- æ–‡å¯¾æ–‡ or è¤‡æ•°æ–‡ ---
        if random.random() < self.multi_prob:
            k = random.randint(1, self.max_k)
            left = max(0, idx - random.randint(0, k))
            right = min(L, idx + random.randint(1, k + 1))
        else:
            left, right = idx, idx + 1

        src = " ".join(self.en[left:right])
        tgt = " ".join(self.ja[left:right])

        if self.add_prefix:
            src = ">>jap<< " + src

        src_tok = self.tok(src, max_length=self.max_len, truncation=True,
                           padding="max_length", return_tensors="pt")
        tgt_tok = self.tok(text_target=tgt, max_length=self.max_len,
                           truncation=True, padding="max_length",
                           return_tensors="pt")

        labels = tgt_tok["input_ids"].clone()
        labels[labels == self.tok.pad_token_id] = -100

        return {
            "input_ids": src_tok["input_ids"].squeeze(),
            "attention_mask": src_tok["attention_mask"].squeeze(),
            "labels": labels.squeeze(),
        }

    def set_multi_prob(self, value: float):
        self.multi_prob = max(0.0, min(1.0, value))


class TranslationDatasetByWorkMemoryEfficient(torch.utils.data.Dataset):
    """
    ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãªByWorkãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
    
    æ”¹å–„ç‚¹:
    1. __init__ã§å…¨ä½œå“ã‚’ãƒ¡ãƒ¢ãƒªã«å±•é–‹ã›ãšã€ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¨ä½ç½®æƒ…å ±ã®ã¿ä¿æŒ
    2. __getitem__ã§å¿…è¦ãªæ™‚ã«å¿…è¦ãªä½œå“ã ã‘ã‚’æ§‹ç¯‰
    """

    def __init__(self, en_list, ja_list, tokenizer, max_len=1024,
                 sep_en="%%%%%%%%THISWORKENDSHERE%%%%%%%%",
                 sep_ja="%%%%%%%%ã“ã®ä½œå“ã“ã“ã¾ã§%%%%%%%%"):
        self.en_list = en_list  # å…ƒã®ãƒªã‚¹ãƒˆã¸ã®å‚ç…§ã‚’ä¿æŒ
        self.ja_list = ja_list
        self.tok = tokenizer
        self.max_len = max_len
        self.sep_en = sep_en
        self.sep_ja = sep_ja
        self.add_prefix = hasattr(tokenizer, 'supported_language_codes')

        # ä½œå“ã®å¢ƒç•Œã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ã¿ã‚’ä¿å­˜(ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„)
        self.work_boundaries = []  # [(start_idx, end_idx), ...]
        
        start_idx = 0
        for i, (en, ja) in enumerate(zip(en_list, ja_list)):
            if en == self.sep_en and ja == self.sep_ja:
                if i > start_idx:  # ç©ºã®ä½œå“ã‚’é¿ã‘ã‚‹
                    self.work_boundaries.append((start_idx, i))
                start_idx = i + 1
        
        # æœ€å¾Œã®ä½œå“
        if start_idx < len(en_list):
            self.work_boundaries.append((start_idx, len(en_list)))
        
        logger.info(f"  ğŸ“š Found {len(self.work_boundaries)} works in ByWork dataset")

    def __len__(self):
        return len(self.work_boundaries)

    def __getitem__(self, idx):
        # å¿…è¦ãªä½œå“ã®ã¿ã‚’ãã®å ´ã§æ§‹ç¯‰
        start_idx, end_idx = self.work_boundaries[idx]
        
        src = " ".join(self.en_list[start_idx:end_idx])
        tgt = " ".join(self.ja_list[start_idx:end_idx])

        # ç¿»è¨³å…ˆè¨€èªæŒ‡å®šãŒå¿…è¦ãªãƒ¢ãƒ‡ãƒ«ã®å ´åˆ(OPUSç³»ãªã©)
        if self.add_prefix:
            src = ">>jap<< " + src

        # ---- ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚º ----
        src_tok = self.tok(src, max_length=self.max_len, truncation=True,
                           padding="max_length", return_tensors="pt")

        tgt_tok = self.tok(text_target=tgt, max_length=self.max_len,
                           truncation=True, padding="max_length",
                           return_tensors="pt")

        labels = tgt_tok["input_ids"].clone()
        labels[labels == self.tok.pad_token_id] = -100

        return {
            "input_ids": src_tok["input_ids"].squeeze(),
            "attention_mask": src_tok["attention_mask"].squeeze(),
            "labels": labels.squeeze(),
        }


def build_combined_dataset(file_paths, tokenizer, max_len=256,
                          max_samples_per_span_file=None, random_seed=42, tags=None):
    """
    ByWorkç³»ã¨RandomSpanç³»ã‚’é©åˆ‡ã«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã¦çµåˆ
    
    Args:
        file_paths: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆ
        tokenizer: ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼
        max_len: æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³é•·
        max_samples_per_span_file: RandomSpanç³»ã®å„ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å–ã‚‹æœ€å¤§ã‚µãƒ³ãƒ—ãƒ«æ•°
        random_seed: ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰
        tags: å„ãƒ•ã‚¡ã‚¤ãƒ«ã«å¯¾å¿œã™ã‚‹ã‚¿ã‚°ã®ãƒªã‚¹ãƒˆ
    """
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ (ãƒãƒ©ãƒ³ã‚¹èª¿æ•´æ¸ˆã¿)
    bywork_files, span_files = load_datasets_balanced(
        file_paths,
        max_samples_per_type=max_samples_per_span_file,
        random_seed=random_seed,
        tags=tags
    )
    
    datasets = []
    
    # ByWorkç³»ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ(ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãƒãƒ¼ã‚¸ãƒ§ãƒ³ä½¿ç”¨)
    for fp, en_list, ja_list in bywork_files:
        ds = TranslationDatasetByWorkMemoryEfficient(en_list, ja_list, tokenizer, max_len=max_len)
        datasets.append(ds)
        logger.info(f"âœ… Created ByWork dataset from {os.path.basename(fp)}: {len(ds)} works")
    
    # RandomSpanç³»ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
    for fp, en_list, ja_list in span_files:
        ds = TranslationDatasetRandomSpan(en_list, ja_list, tokenizer, max_len=max_len)
        datasets.append(ds)
        logger.info(f"âœ… Created RandomSpan dataset from {os.path.basename(fp)}: {len(ds)} pairs")
    
    # è¤‡æ•° dataset ã‚’é€£çµ
    from torch.utils.data import ConcatDataset
    combined = ConcatDataset(datasets)
    logger.info(f"\nğŸ¯ Combined dataset total size: {len(combined)}")
    
    return combined


# ===============================
# 3. æ¤œè¨¼é–¢æ•°
# ===============================
def evaluate_model(model, val_loader, device):
    model.eval()
    total_loss = 0
    with torch.no_grad():
        for batch in val_loader:
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            labels = batch["labels"].to(device)
            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
            total_loss += outputs.loss.item()
    return total_loss / len(val_loader)


# ===============================
# 4. Early Stopping
# ===============================
class EarlyStopping:

    def __init__(self, patience=3, min_delta=0.001):
        self.patience = patience
        self.min_delta = min_delta
        self.counter = 0
        self.best_loss = None
        self.early_stop = False
    
    def __call__(self, val_loss):
        if self.best_loss is None:
            self.best_loss = val_loss
        elif val_loss > self.best_loss - self.min_delta:
            self.counter += 1
            logger.info(f"âš ï¸ No improvement for {self.counter} epoch(s)")
            if self.counter >= self.patience:
                self.early_stop = True
        else:
            self.best_loss = val_loss
            self.counter = 0


def freeze_encoder_layers(model, ratio=0.5):
    enc_layers = model.model.encoder.layers
    freeze_until = int(len(enc_layers) * ratio)
    for i, layer in enumerate(enc_layers):
        for p in layer.parameters():
            p.requires_grad = i >= freeze_until


# ===============================
# 5. é«˜é€ŸåŒ–ã•ã‚ŒãŸå­¦ç¿’é–¢æ•°
# ===============================
def train_model(
    model_name,
    file_paths,
    epochs=3,
    batch_size=32,
    use_amp=True,
    max_samples_per_span_file=None,
    val_split=0.05,
    save_dir="./models",
    learning_rate=1e-4,
    gradient_clip=1.0,
    save_every=1,
    patience=2,
    max_len=64,
    random_seed=42,
    tags=None,
    # ğŸ†• é«˜é€ŸåŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    num_workers=4,  # DataLoaderã®ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°
    accumulation_steps=4,  # Gradient Accumulation
    use_bfloat16=True,  # BFloat16ã‚’ä½¿ç”¨ã™ã‚‹ã‹
    use_compile=True,  # torch.compileã‚’ä½¿ç”¨ã™ã‚‹ã‹
    scheduler_type='onecycle',  # 'onecycle' or 'linear_warmup'
    warmup_steps=500  # linear_warmupç”¨ã®ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ã‚¹ãƒ†ãƒƒãƒ—æ•°
):
    """
    æœ€é©åŒ–ã•ã‚ŒãŸå­¦ç¿’é–¢æ•°
    
    Args:
        scheduler_type: 'onecycle' (OneCycleLR) ã¾ãŸã¯ 'linear_warmup' (get_linear_schedule_with_warmup)
        warmup_steps: linear_warmupä½¿ç”¨æ™‚ã®ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ã‚¹ãƒ†ãƒƒãƒ—æ•°
    """
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    logger.info(f"ğŸš€ Using device: {device}")
    
    # ğŸ†• é«˜é€ŸåŒ–è¨­å®šã®ãƒ­ã‚°å‡ºåŠ›
    logger.info("\n" + "="*60)
    logger.info("âš¡ SPEED OPTIMIZATION SETTINGS")
    logger.info("="*60)
    logger.info(f"âœ“ DataLoader workers: {num_workers}")
    logger.info(f"âœ“ Gradient accumulation steps: {accumulation_steps}")
    logger.info(f"âœ“ Effective batch size: {batch_size * accumulation_steps}")
    logger.info(f"âœ“ BFloat16: {use_bfloat16 and device.type == 'cuda'}")
    logger.info(f"âœ“ torch.compile: {use_compile}")
    logger.info(f"âœ“ Scheduler type: {scheduler_type}")
    if scheduler_type == 'linear_warmup':
        logger.info(f"âœ“ Warmup steps: {warmup_steps}")
    logger.info("="*60 + "\n")
    
    tokenizer = AutoTokenizer.from_pretrained(model_name, use_safetensors=True)
    model = AutoModelForSeq2SeqLM.from_pretrained(model_name, use_safetensors=True).to(device)
    model.config.dropout = 0.15
    model.config.attention_dropout = 0.15
    
    # ğŸ†• torch.compile (PyTorch 2.0+)
    # Transformersã¨ã®äº’æ›æ€§å•é¡Œã‚’å›é¿ã™ã‚‹ãŸã‚ã€ã‚¨ãƒ©ãƒ¼æŠ‘åˆ¶ã‚’æœ‰åŠ¹åŒ–
    """
    if use_compile and hasattr(torch, 'compile'):
        logger.info("ğŸ”¥ Compiling model with torch.compile...")
        try:
            import torch._dynamo
            torch._dynamo.config.suppress_errors = True  # Transformersäº’æ›æ€§ã®ãŸã‚
            model = torch.compile(model, mode='reduce-overhead')
            logger.info("âœ… Model compiled successfully!")
        except Exception as e:
            logger.warning(f"âš ï¸  torch.compile failed: {e}. Continuing without compilation.")
            use_compile = False
    """
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ§‹ç¯‰
    dataset = build_combined_dataset(
        file_paths,
        tokenizer,
        max_len=max_len,
        max_samples_per_span_file=max_samples_per_span_file,
        random_seed=random_seed,
        tags=tags
    )
    val_size = int(len(dataset) * val_split)
    train_size = len(dataset) - val_size
    train_dataset, val_dataset = random_split(
        dataset, [train_size, val_size],
        generator=torch.Generator().manual_seed(random_seed)
    )

    # ã¾ãš ConcatDataset å…¨ä½“ã‹ã‚‰ index ã‚’ä½œã‚‹
    bywork_idx, span_idx = split_concat_dataset(dataset)

    # æ¬¡ã« train / val åˆ†å‰²
    val_size = int(len(dataset) * val_split)
    train_size = len(dataset) - val_size
    train_dataset, val_dataset = random_split(
        dataset, [train_size, val_size],
        generator=torch.Generator().manual_seed(random_seed)
    )
    train_indices = set(train_dataset.indices)

    train_bywork_idx = [i for i in bywork_idx if i in train_indices]
    train_span_idx = [i for i in span_idx   if i in train_indices]

    train_bywork = Subset(dataset, train_bywork_idx)
    train_span = Subset(dataset, train_span_idx)
    
    logger.info(f"\nğŸ“Š Dataset split:")
    logger.info(f"  Training: {train_size:,} samples")
    logger.info(f"  Validation: {val_size:,} samples\n")
    
    # ğŸ†• DataLoaderã®æœ€é©åŒ–
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,  # ãƒãƒ«ãƒãƒ—ãƒ­ã‚»ã‚¹èª­ã¿è¾¼ã¿
        pin_memory=True,  # GPUè»¢é€ã®é«˜é€ŸåŒ–
        prefetch_factor=2,  # å…ˆèª­ã¿ãƒãƒƒãƒ•ã‚¡
        persistent_workers=True if num_workers > 0 else False  # ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ—ãƒ­ã‚»ã‚¹ã‚’ç¶­æŒ
    )

    span_collator = build_randomspan_collator(tokenizer)

    train_loader_span = DataLoader(
        train_span,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        collate_fn=span_collator,
        pin_memory=True,
        persistent_workers=True
    )

    train_loader_bywork = DataLoader(
        train_bywork,
        batch_size=max(1, batch_size // 4),  # é•·æ–‡ãªã®ã§å°ã•ã‚
        shuffle=True,
        num_workers=num_workers,
        pin_memory=True,
        persistent_workers=True
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size * 2,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=True,
        prefetch_factor=2,
        persistent_workers=True if num_workers > 0 else False
    )
    
    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)
    
    # ğŸ†• ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã®é¸æŠ
    scheduler = None
    if scheduler_type == 'onecycle':
        total_steps = len(train_loader) * epochs // accumulation_steps
        scheduler = OneCycleLR(
            optimizer,
            max_lr=learning_rate * 10,  # æœ€å¤§å­¦ç¿’ç‡
            total_steps=total_steps,
            pct_start=0.3,  # ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ã®å‰²åˆ
            anneal_strategy='cos',
            div_factor=25.0,  # åˆæœŸå­¦ç¿’ç‡ = max_lr / div_factor
            final_div_factor=1e4  # æœ€çµ‚å­¦ç¿’ç‡ = max_lr / final_div_factor
        )
        logger.info(f"ğŸ“ˆ OneCycleLR scheduler initialized (total_steps={total_steps})")
    elif scheduler_type == 'linear_warmup':
        num_training_steps = (len(train_loader) // accumulation_steps) * epochs
        scheduler = get_linear_schedule_with_warmup(
            optimizer,
            num_warmup_steps=warmup_steps,
            num_training_steps=num_training_steps
        )
        logger.info(f"ğŸ“ˆ Linear warmup scheduler initialized (warmup_steps={warmup_steps}, total_steps={num_training_steps})")
    
    # ğŸ†• BFloat16ã‚µãƒãƒ¼ãƒˆãƒã‚§ãƒƒã‚¯
    use_bf16 = use_bfloat16 and device.type == "cuda" and torch.cuda.is_bf16_supported()
    if use_bfloat16 and not use_bf16:
        logger.warning("âš ï¸  BFloat16 requested but not supported. Falling back to FP16.")
    
    scaler = GradScaler() if use_amp and device.type == "cuda" and not use_bf16 else None
    early_stopping = EarlyStopping(patience=patience)
    
    best_val_loss = float('inf')
    os.makedirs(save_dir, exist_ok=True)
    freeze_encoder_layers(model, ratio=0.5)

    for epoch in range(epochs):
        start_prob = 0.5
        end_prob = 0.1
        current_prob = start_prob + (end_prob - start_prob) * (epoch / max(1, epochs - 1))
        for ds in dataset.datasets:
            if isinstance(ds, TranslationDatasetRandomSpan):
                ds.set_multi_prob(current_prob)

        logger.info(f"ğŸ“‰ RandomSpan multi_prob = {current_prob:.2f}")

        model.train()
        total_loss = 0
        loaders = [train_loader_span, train_loader_bywork]
        for loader in loaders:
            pbar = tqdm(loader, desc=f"Epoch {epoch+1}/{epochs}")

            for batch_idx, batch in enumerate(pbar):

                # accumulation ã®å…ˆé ­ã§ zero_grad
                if batch_idx % accumulation_steps == 0:
                    optimizer.zero_grad(set_to_none=True)

                input_ids = batch["input_ids"].to(device)
                attention_mask = batch["attention_mask"].to(device)
                labels = batch["labels"].to(device)

                if use_bf16:
                    with autocast(dtype=torch.bfloat16):
                        outputs = model(
                            input_ids=input_ids,
                            attention_mask=attention_mask,
                            labels=labels
                        )
                        loss = outputs.loss / accumulation_steps
                    loss.backward()

                elif scaler:
                    with autocast():
                        outputs = model(
                            input_ids=input_ids,
                            attention_mask=attention_mask,
                            labels=labels
                        )
                        loss = outputs.loss / accumulation_steps
                    scaler.scale(loss).backward()

                else:
                    outputs = model(
                        input_ids=input_ids,
                        attention_mask=attention_mask,
                        labels=labels
                    )
                    loss = outputs.loss / accumulation_steps
                    loss.backward()

                # â˜… ã“ã“ã‹ã‚‰ã€Œæ›´æ–°ãƒ•ã‚§ãƒ¼ã‚ºã€
                if (batch_idx + 1) % accumulation_steps == 0:

                    if gradient_clip > 0:
                        if scaler:
                            scaler.unscale_(optimizer)
                        torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clip)

                    if scaler:
                        scaler.step(optimizer)
                        scaler.update()
                    else:
                        optimizer.step()

                    if scheduler:
                        scheduler.step()

                # â˜… ãƒ­ã‚°ç”¨æå¤±ã¯æ¯ãƒãƒƒãƒ
                total_loss += loss.item() * accumulation_steps

                current_lr = optimizer.param_groups[0]["lr"]
                pbar.set_postfix(
                    loss=f"{loss.item() * accumulation_steps:.4f}",
                    lr=f"{current_lr:.2e}"
                )

        if epoch == 1:
            for p in model.parameters():
                p.requires_grad = True
            logger.info("ğŸ”“ Encoder fully unfrozen")

        # ã‚¨ãƒãƒƒã‚¯çµ‚äº†æ™‚ã®æ¤œè¨¼
        val_loss = evaluate_model(model, val_loader, device)
        logger.info(f"ğŸ“Š Epoch {epoch+1}/{epochs} - Validation loss: {val_loss:.4f}")
        
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            model.save_pretrained(os.path.join(save_dir, "best_model"))
            tokenizer.save_pretrained(os.path.join(save_dir, "best_model"))
            logger.info("â­ New best model saved!")
        
        early_stopping(val_loss)
        if early_stopping.early_stop:
            logger.info(f"ğŸ›‘ Early stopping triggered at epoch {epoch+1}")
            break
    
    logger.info(f"\nâœ… Training completed! Best validation loss: {best_val_loss:.4f}")
    return model, tokenizer


# ===============================
# 6. ç¿»è¨³é–¢æ•°
# ===============================
def translate(model, tokenizer, text, max_length=64, num_beams=4):
    if hasattr(tokenizer, 'supported_language_codes'):
        text = ">>jap<< " + text
    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=max_length)
    device = next(model.parameters()).device
    inputs = {k: v.to(device) for k, v in inputs.items()}
    with torch.no_grad():
        outputs = model.generate(**inputs, max_length=max_length, num_beams=num_beams)
    return tokenizer.decode(outputs[0], skip_special_tokens=True)


def batch_translate(model, tokenizer, texts, batch_size=8, max_length=64, num_beams=4):
    device = next(model.parameters()).device
    if hasattr(tokenizer, 'supported_language_codes'):
        texts = [">>jap<< " + t for t in texts]
    translations = []
    for i in range(0, len(texts), batch_size):
        batch_texts = texts[i:i + batch_size]
        inputs = tokenizer(batch_texts, return_tensors="pt", padding=True, truncation=True, max_length=max_length)
        inputs = {k: v.to(device) for k, v in inputs.items()}
        with torch.no_grad():
            outputs = model.generate(**inputs, max_length=max_length, num_beams=num_beams)
        translations.extend([tokenizer.decode(o, skip_special_tokens=True) for o in outputs])
    return translations


# ===============================
# å®Ÿè¡Œä¾‹
# ===============================
if __name__ == "__main__":
    files = [
        "./../data/sepalated_dataset.jsonl",  # ByWorkç³»
        "./../data/OpenSubtitles_sample_40000.jsonl",  # RandomSpanç³»
        "./../data/TED_sample_40000.jsonl",  # RandomSpanç³»
        "./../data/Tatoeba_sample_40000.jsonl",  # RandomSpanç³»
        "./../data/all_outenjp.jsonl"  # RandomSpanç³» (æ­Œè©ãªã©)
    ]
    
    # ğŸ†• å„ãƒ•ã‚¡ã‚¤ãƒ«ã«å¯¾å¿œã™ã‚‹ã‚¿ã‚° (å¿…è¦ã«å¿œã˜ã¦è¨­å®š)
    # tags = [None, None, None, None, "[LYRICS]"]  # æ­Œè©ãƒ‡ãƒ¼ã‚¿ã«ã‚¿ã‚°ã‚’ä»˜ã‘ã‚‹ä¾‹
    tags = None  # ã‚¿ã‚°ãªã—ã®å ´åˆ
   
    MODEL_NAME = "Helsinki-NLP/opus-mt-en-jap"
    SAVE_DIR = "./models/translation_model_final"
    
    # === OneCycleLR ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã‚’ä½¿ã†å ´åˆ ===
    model, tokenizer = train_model(
        MODEL_NAME,
        files,
        epochs=2,
        batch_size=16,
        max_samples_per_span_file=40000,
        save_dir=SAVE_DIR,
        random_seed=42,
        tags=tags,
        # é«˜é€ŸåŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        num_workers=4,
        accumulation_steps=4,
        use_bfloat16=True,
        use_compile=True,
        scheduler_type='onecycle'  # OneCycleLRä½¿ç”¨
    )
    
    # === Linear Warmup ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã‚’ä½¿ã†å ´åˆ ===
    # model, tokenizer = train_model(
    #     MODEL_NAME,
    #     files,
    #     epochs=2,
    #     batch_size=16,
    #     max_samples_per_span_file=40000,
    #     save_dir=SAVE_DIR,
    #     random_seed=42,
    #     tags=tags,
    #     # é«˜é€ŸåŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    #     num_workers=4,
    #     accumulation_steps=4,
    #     use_bfloat16=True,
    #     use_compile=True,
    #     scheduler_type='linear_warmup',  # Linear Warmupä½¿ç”¨
    #     warmup_steps=500
    # )
    
    test_sentences = [
        "I like apples.",
        "How are you?",
        "Machine learning is fun.",
        "I couldn't speak English well."
    ]
    results = batch_translate(model, tokenizer, test_sentences)
    for en, ja in zip(test_sentences, results):
        print(f"EN: {en} -> JA: {ja}")
