import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, get_linear_schedule_with_warmup, DataCollatorForSeq2Seq
from torch.cuda.amp import autocast, GradScaler
from torch.utils.data import DataLoader, random_split
from torch.optim.lr_scheduler import OneCycleLR
from tqdm import tqdm
import random
from torch.utils.data import Dataset, Subset
import os
from pathlib import Path
import json
import logging
from dataclasses import dataclass, field
from typing import List, Optional

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


# ===============================
# 0. è¨­å®šã‚¯ãƒ©ã‚¹
# ===============================
@dataclass
class TrainingConfig:
    model_name: str
    file_paths: List[str]
    epochs: int = 3
    batch_size: int = 32
    use_amp: bool = True
    max_samples_per_span_file: Optional[int] = None
    val_split: float = 0.05
    save_dir: str = "./models"
    learning_rate: float = 1e-4
    gradient_clip: float = 1.0
    patience: int = 2
    max_len: int = 64
    random_seed: int = 42
    tags: Optional[List[str]] = None
    num_workers: int = 4
    accumulation_steps: int = 4
    use_bfloat16: bool = True
    use_compile: bool = False
    scheduler_type: str = 'onecycle'
    warmup_steps: int = 500
    
    # ğŸ†• ãƒ¢ãƒƒã‚¯ãƒ¢ãƒ¼ãƒ‰è¨­å®š
    mock_mode: bool = False
    mock_samples: int = 100  # ãƒ¢ãƒƒã‚¯æ™‚ã®ã‚µãƒ³ãƒ—ãƒ«æ•°
    mock_force_cpu: bool = True  # ãƒ¢ãƒƒã‚¯æ™‚ã¯å¼·åˆ¶çš„ã«CPUä½¿ç”¨


# ===============================
# ğŸ†• ãƒ¢ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
# ===============================
def generate_mock_data(num_samples=100, seed=42):
    """
    ãƒ†ã‚¹ãƒˆç”¨ã®ãƒ¢ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
    
    Args:
        num_samples: ç”Ÿæˆã™ã‚‹ã‚µãƒ³ãƒ—ãƒ«æ•°
        seed: ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰
    
    Returns:
        en_list, ja_list
    """
    random.seed(seed)
    
    # ã‚·ãƒ³ãƒ—ãƒ«ãªè‹±æ—¥å¯¾è¨³ã®ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
    templates = [
        ("I like {}", "ç§ã¯{}ãŒå¥½ãã§ã™"),
        ("This is a {}", "ã“ã‚Œã¯{}ã§ã™"),
        ("How is the {}?", "{}ã¯ã©ã†ã§ã™ã‹?"),
        ("I want to eat {}", "{}ã‚’é£Ÿã¹ãŸã„ã§ã™"),
        ("The {} is beautiful", "ãã®{}ã¯ç¾ã—ã„"),
        ("I can see a {}", "{}ãŒè¦‹ãˆã¾ã™"),
        ("Where is the {}?", "{}ã¯ã©ã“ã§ã™ã‹?"),
        ("I need a {}", "{}ãŒå¿…è¦ã§ã™"),
    ]
    
    words = ["apple", "book", "car", "dog", "house", "computer", "phone", "music", 
             "movie", "game", "coffee", "tea", "flower", "bird", "cat", "tree"]
    
    en_list, ja_list = [], []
    
    for i in range(num_samples):
        template_en, template_ja = random.choice(templates)
        word = random.choice(words)
        
        en_list.append(template_en.format(word))
        ja_list.append(template_ja.format(word))
    
    logger.info(f"ğŸ­ Generated {len(en_list)} mock samples")
    return en_list, ja_list


def create_mock_jsonl_files(output_dir="./mock_data", num_files=2, samples_per_file=50):
    """
    ãƒ¢ãƒƒã‚¯ç”¨ã®JSONLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆ
    
    Args:
        output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        num_files: ç”Ÿæˆã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«æ•°
        samples_per_file: ãƒ•ã‚¡ã‚¤ãƒ«ã”ã¨ã®ã‚µãƒ³ãƒ—ãƒ«æ•°
    
    Returns:
        ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆ
    """
    os.makedirs(output_dir, exist_ok=True)
    file_paths = []
    
    for i in range(num_files):
        file_path = os.path.join(output_dir, f"mock_data_{i+1}.jsonl")
        en_list, ja_list = generate_mock_data(samples_per_file, seed=42 + i)
        
        with open(file_path, "w", encoding="utf-8") as f:
            for en, ja in zip(en_list, ja_list):
                json.dump({"en": en, "ja": ja}, f, ensure_ascii=False)
                f.write("\n")
        
        file_paths.append(file_path)
        logger.info(f"âœ… Created mock file: {file_path}")
    
    return file_paths


# ===============================
# 1. ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãªãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
# ===============================
def add_tag_if_needed(text, tag):
    """ã‚¿ã‚°ã‚’è¿½åŠ ã™ã‚‹ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°"""
    return f"{tag} {text}" if tag else text


def load_single_dataset_streaming(file_path, max_samples=None, random_seed=42, tag=None):
    """
    ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ã«JSONLãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
    å…¨è¡Œã‚’ä¸€åº¦ã«ãƒ¡ãƒ¢ãƒªã«èª­ã¿è¾¼ã¾ãšã€å¿…è¦ãªåˆ†ã ã‘å‡¦ç†
    
    Args:
        file_path: èª­ã¿è¾¼ã‚€ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        max_samples: æœ€å¤§ã‚µãƒ³ãƒ—ãƒ«æ•°
        random_seed: ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰
        tag: è‹±æ–‡ã®å…ˆé ­ã«è¿½åŠ ã™ã‚‹ã‚¿ã‚° (ä¾‹: "[LYRICS]")
    """
    en_list, ja_list = [], []
    error_count = 0
    
    logger.info(f"ğŸ“– Loading {file_path} ...")
    
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            # max_samplesãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆ
            if max_samples:
                # ã¾ãšå…¨è¡Œæ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ(ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ã«)
                total_lines = sum(1 for _ in f)
                f.seek(0)  # ãƒ•ã‚¡ã‚¤ãƒ«ãƒã‚¤ãƒ³ã‚¿ã‚’å…ˆé ­ã«æˆ»ã™
                
                if total_lines > max_samples:
                    logger.info(f"  âš¡ Sampling {max_samples} from {total_lines} lines")
                    # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã™ã‚‹è¡Œç•ªå·ã‚’äº‹å‰ã«æ±ºå®š
                    random.seed(random_seed)
                    selected_indices = set(random.sample(range(total_lines), max_samples))
                    
                    # é¸æŠã•ã‚ŒãŸè¡Œã ã‘ã‚’å‡¦ç†
                    for idx, line in enumerate(tqdm(f, total=total_lines,
                                                    desc=f"Reading {os.path.basename(file_path)}",
                                                    unit=" lines")):
                        if idx in selected_indices:
                            try:
                                data = json.loads(line)
                                en, ja = data.get("en"), data.get("ja")
                                if en and ja and len(en.strip()) > 0 and len(ja.strip()) > 0:
                                    en_list.append(add_tag_if_needed(en, tag))
                                    ja_list.append(ja)
                            except json.JSONDecodeError:
                                error_count += 1
                else:
                    # å…¨è¡Œã‚’å‡¦ç†
                    f.seek(0)
                    for line in tqdm(f, total=total_lines,
                                   desc=f"Reading {os.path.basename(file_path)}",
                                   unit=" lines"):
                        try:
                            data = json.loads(line)
                            en, ja = data.get("en"), data.get("ja")
                            if en and ja and len(en.strip()) > 0 and len(ja.strip()) > 0:
                                en_list.append(add_tag_if_needed(en, tag))
                                ja_list.append(ja)
                        except json.JSONDecodeError:
                            error_count += 1
            else:
                # max_samplesãªã—ã®å ´åˆã¯å…¨è¡Œã‚’å‡¦ç†
                for line in tqdm(f, desc=f"Reading {os.path.basename(file_path)}", unit=" lines"):
                    try:
                        data = json.loads(line)
                        en, ja = data.get("en"), data.get("ja")
                        if en and ja and len(en.strip()) > 0 and len(ja.strip()) > 0:
                            en_list.append(add_tag_if_needed(en, tag))
                            ja_list.append(ja)
                    except json.JSONDecodeError:
                        error_count += 1
    
    except FileNotFoundError:
        logger.error(f"âŒ File not found: {file_path}")
        return [], []
    except Exception as e:
        logger.error(f"âŒ Unexpected error loading {file_path}: {e}")
        return [], []
    
    if error_count > 0:
        logger.warning(f"  âš ï¸  Skipped {error_count} invalid lines")
    
    logger.info(f"  âœ… Loaded {len(en_list)} pairs from {os.path.basename(file_path)}")
    return en_list, ja_list


def load_datasets_balanced(file_paths, max_samples_per_type=None, random_seed=42, tags=None):
    """
    ByWorkç³»ã¨RandomSpanç³»ã‚’åˆ†ã‘ã¦ã€ãã‚Œãã‚Œã‹ã‚‰é©åˆ‡ã«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    
    Args:
        file_paths: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆ
        max_samples_per_type: RandomSpanç³»ã®å„ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å–å¾—ã™ã‚‹æœ€å¤§ã‚µãƒ³ãƒ—ãƒ«æ•°
        random_seed: ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰
        tags: å„ãƒ•ã‚¡ã‚¤ãƒ«ã«å¯¾å¿œã™ã‚‹ã‚¿ã‚°ã®ãƒªã‚¹ãƒˆ (Noneã®å ´åˆã¯ã‚¿ã‚°ãªã—)
    
    Returns:
        bywork_files: [(file_path, en_list, ja_list), ...]
        span_files: [(file_path, en_list, ja_list), ...]
    """
    bywork_files = []
    span_files = []
    
    # tagsãŒNoneã®å ´åˆã¯å…¨ã¦Noneã®ãƒªã‚¹ãƒˆã‚’ä½œæˆ
    if tags is None:
        tags = [None] * len(file_paths)
    
    for fp, tag in zip(file_paths, tags):
        is_bywork = "separated" in Path(fp).name or "sepalated" in Path(fp).name
        
        if is_bywork:
            # ByWorkç³»ã¯å…¨ã¦èª­ã¿è¾¼ã‚€
            logger.info(f"\nğŸ¯ [WORK-LEVEL] {fp} (loading ALL)")
            en_list, ja_list = load_single_dataset_streaming(fp, max_samples=None, random_seed=random_seed, tag=tag)
            bywork_files.append((fp, en_list, ja_list))
        else:
            # RandomSpanç³»ã¯max_samples_per_typeåˆ†ã ã‘
            logger.info(f"\nğŸ² [SPAN-LEVEL] {fp}")
            en_list, ja_list = load_single_dataset_streaming(fp, max_samples=max_samples_per_type, random_seed=random_seed, tag=tag)
            span_files.append((fp, en_list, ja_list))
    
    # ã‚µãƒãƒªãƒ¼è¡¨ç¤º
    logger.info("\n" + "="*60)
    logger.info("ğŸ“Š LOADING SUMMARY")
    logger.info("="*60)
    
    total_bywork = sum(len(data[1]) for data in bywork_files)
    logger.info(f"ByWork datasets: {len(bywork_files)} files, {total_bywork:,} pairs total")
    for fp, en_list, _ in bywork_files:
        logger.info(f"  - {os.path.basename(fp)}: {len(en_list):,} pairs")
    
    total_span = sum(len(data[1]) for data in span_files)
    logger.info(f"\nRandomSpan datasets: {len(span_files)} files, {total_span:,} pairs total")
    for fp, en_list, _ in span_files:
        logger.info(f"  - {os.path.basename(fp)}: {len(en_list):,} pairs")
    
    logger.info(f"\nğŸ‰ GRAND TOTAL: {total_bywork + total_span:,} pairs")
    logger.info("="*60 + "\n")
    
    return bywork_files, span_files


# ===============================
# 2. ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãª Dataset ã‚¯ãƒ©ã‚¹
# ===============================

# RandomSpan ç”¨ collator(dynamic padding + label smoothing)
def build_randomspan_collator(tokenizer, label_smoothing=0.1):
    return DataCollatorForSeq2Seq(
        tokenizer=tokenizer,
        model=None,
        label_pad_token_id=-100,
        padding=True,
    )


class TranslationDatasetRandomSpan(Dataset):
    """RandomSpanç³»ãƒ‡ãƒ¼ã‚¿ç”¨Dataset"""
    def __init__(self, en_texts, ja_texts, tokenizer, max_len=64, multi_prob=0.5):
        self.en_texts = en_texts
        self.ja_texts = ja_texts
        self.tokenizer = tokenizer
        self.max_len = max_len
        self.multi_prob = multi_prob

    def __len__(self):
        return len(self.en_texts)

    def set_multi_prob(self, prob):
        self.multi_prob = prob

    def __getitem__(self, idx):
        en_text = self.en_texts[idx]
        ja_text = self.ja_texts[idx]
        
        if hasattr(self.tokenizer, 'supported_language_codes'):
            en_text = ">>jap<< " + en_text

        # ãƒãƒ«ãƒã‚»ãƒ³ãƒ†ãƒ³ã‚¹åŒ–
        if random.random() < self.multi_prob and idx + 1 < len(self.en_texts):
            en_text = en_text + " " + self.en_texts[idx + 1]
            ja_text = ja_text + " " + self.ja_texts[idx + 1]

        inputs = self.tokenizer(en_text, max_length=self.max_len, truncation=True, padding=False)
        labels = self.tokenizer(ja_text, max_length=self.max_len, truncation=True, padding=False)

        return {
            "input_ids": inputs["input_ids"],
            "attention_mask": inputs["attention_mask"],
            "labels": labels["input_ids"]
        }


class TranslationDatasetByWork(Dataset):
    """ByWorkç³»ãƒ‡ãƒ¼ã‚¿ç”¨Dataset (ã‚·ãƒ³ãƒ—ãƒ«ç‰ˆ)"""
    def __init__(self, en_texts, ja_texts, tokenizer, max_len=64):
        self.en_texts = en_texts
        self.ja_texts = ja_texts
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.en_texts)

    def __getitem__(self, idx):
        en_text = self.en_texts[idx]
        ja_text = self.ja_texts[idx]
        
        if hasattr(self.tokenizer, 'supported_language_codes'):
            en_text = ">>jap<< " + en_text

        inputs = self.tokenizer(en_text, max_length=self.max_len, truncation=True, padding="max_length")
        labels = self.tokenizer(ja_text, max_length=self.max_len, truncation=True, padding="max_length")

        return {
            "input_ids": inputs["input_ids"],
            "attention_mask": inputs["attention_mask"],
            "labels": labels["input_ids"]
        }


# ===============================
# 3. Early Stopping
# ===============================
class EarlyStopping:
    def __init__(self, patience=2):
        self.patience = patience
        self.counter = 0
        self.best_loss = float('inf')
        self.early_stop = False

    def __call__(self, val_loss):
        if val_loss < self.best_loss:
            self.best_loss = val_loss
            self.counter = 0
        else:
            self.counter += 1
            if self.counter >= self.patience:
                self.early_stop = True


# ===============================
# 4. å­¦ç¿’è£œåŠ©é–¢æ•°
# ===============================
def freeze_encoder_layers(model, ratio=0.5):
    """Encoderã®ä¸€éƒ¨ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’å‡çµ"""
    if hasattr(model, 'model') and hasattr(model.model, 'encoder'):
        encoder = model.model.encoder
    elif hasattr(model, 'encoder'):
        encoder = model.encoder
    else:
        logger.warning("âš ï¸ Could not find encoder to freeze")
        return

    if hasattr(encoder, 'layers'):
        total_layers = len(encoder.layers)
        freeze_count = int(total_layers * ratio)
        for i, layer in enumerate(encoder.layers):
            if i < freeze_count:
                for param in layer.parameters():
                    param.requires_grad = False
        logger.info(f"ğŸ”’ Frozen {freeze_count}/{total_layers} encoder layers")


def evaluate_model(model, val_loader, device):
    """æ¤œè¨¼ãƒ«ãƒ¼ãƒ—"""
    model.eval()
    total_loss = 0
    with torch.no_grad():
        for batch in val_loader:
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            labels = batch["labels"].to(device)
            
            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
            total_loss += outputs.loss.item()
    
    return total_loss / len(val_loader)


# ===============================
# 5. å­¦ç¿’ãƒ¡ã‚¤ãƒ³å‡¦ç†
# ===============================
def setup_training(config: TrainingConfig):
    """ãƒ‡ãƒã‚¤ã‚¹ã€ãƒ¢ãƒ‡ãƒ«ã€ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®åˆæœŸåŒ–"""
    # ğŸ†• ãƒ¢ãƒƒã‚¯ãƒ¢ãƒ¼ãƒ‰æ™‚ã®å‡¦ç†
    if config.mock_mode:
        logger.info("ğŸ­ " + "="*60)
        logger.info("ğŸ­ MOCK MODE ENABLED - Running with synthetic data")
        logger.info("ğŸ­ " + "="*60)
        
        if config.mock_force_cpu:
            device = torch.device("cpu")
            logger.info("ğŸ­ Forcing CPU for mock mode")
        else:
            device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    else:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    logger.info(f"ğŸ”§ Device: {device}")
    
    # ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®èª­ã¿è¾¼ã¿
    tokenizer = AutoTokenizer.from_pretrained(config.model_name)
    model = AutoModelForSeq2SeqLM.from_pretrained(config.model_name).to(device)
    
    # torch.compile (ã‚ªãƒ—ã‚·ãƒ§ãƒ³)
    if config.use_compile and hasattr(torch, 'compile'):
        try:
            model = torch.compile(model)
            logger.info("âš¡ Model compiled with torch.compile")
        except Exception as e:
            logger.warning(f"âš ï¸ torch.compile failed: {e}")
    
    return device, tokenizer, model


def create_dataloaders(config: TrainingConfig, tokenizer):
    """ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã®ä½œæˆ"""
    # ğŸ†• ãƒ¢ãƒƒã‚¯ãƒ¢ãƒ¼ãƒ‰æ™‚ã¯ãƒ¢ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
    if config.mock_mode:
        logger.info(f"ğŸ­ Generating {config.mock_samples} mock samples...")
        en_list, ja_list = generate_mock_data(config.mock_samples, seed=config.random_seed)
        
        # ãƒ¢ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’RandomSpanå½¢å¼ã¨ã—ã¦æ‰±ã†
        span_files = [("mock_data", en_list, ja_list)]
        bywork_files = []
    else:
        # é€šå¸¸ã®ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        bywork_files, span_files = load_datasets_balanced(
            config.file_paths,
            max_samples_per_type=config.max_samples_per_span_file,
            random_seed=config.random_seed,
            tags=config.tags
        )
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
    all_datasets = []
    
    # RandomSpanç³»
    for _, en_list, ja_list in span_files:
        ds = TranslationDatasetRandomSpan(en_list, ja_list, tokenizer, max_len=config.max_len)
        all_datasets.append(ds)
    
    # ByWorkç³»
    for _, en_list, ja_list in bywork_files:
        ds = TranslationDatasetByWork(en_list, ja_list, tokenizer, max_len=config.max_len)
        all_datasets.append(ds)
    
    if not all_datasets:
        raise ValueError("âŒ No data loaded! Check file paths.")
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®çµåˆ
    from torch.utils.data import ConcatDataset
    dataset = ConcatDataset(all_datasets)
    
    # Train/Valåˆ†å‰²
    val_size = int(len(dataset) * config.val_split)
    train_size = len(dataset) - val_size
    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])
    
    logger.info(f"ğŸ“Š Train: {train_size:,} samples, Validation: {val_size:,} samples")
    
    # RandomSpanã¨ByWorkã‚’åˆ†é›¢
    span_indices = []
    bywork_indices = []
    offset = 0
    
    for i, ds in enumerate(all_datasets):
        ds_len = len(ds)
        if isinstance(ds, TranslationDatasetRandomSpan):
            span_indices.extend(range(offset, offset + ds_len))
        else:
            bywork_indices.extend(range(offset, offset + ds_len))
        offset += ds_len
    
    train_span_indices = [i for i in train_dataset.indices if i in span_indices]
    train_bywork_indices = [i for i in train_dataset.indices if i in bywork_indices]
    
    train_span = Subset(dataset, train_span_indices)
    train_bywork = Subset(dataset, train_bywork_indices)
    
    # Collator
    span_collator = build_randomspan_collator(tokenizer, label_smoothing=0.1)
    
    # DataLoaderè¨­å®š
    loader_args = {
        'num_workers': 0 if config.mock_mode else config.num_workers,  # ğŸ†• ãƒ¢ãƒƒã‚¯æ™‚ã¯num_workers=0
        'pin_memory': False if config.mock_mode else True,  # ğŸ†• ãƒ¢ãƒƒã‚¯æ™‚ã¯pin_memoryç„¡åŠ¹
        'persistent_workers': False
    }
    
    # ğŸ†• ãƒãƒƒãƒã‚µã‚¤ã‚ºã®å®‰å…¨æ€§ãƒã‚§ãƒƒã‚¯
    actual_batch_size = config.batch_size
    bywork_batch_size = max(1, config.batch_size // 4)
    
    if config.accumulation_steps > len(train_span) // actual_batch_size:
        logger.warning(f"âš ï¸ accumulation_steps ({config.accumulation_steps}) is large relative to dataset size")
    
    train_loader_span = DataLoader(train_span, batch_size=actual_batch_size, shuffle=True, collate_fn=span_collator, **loader_args)
    
    # ğŸ†• ByWorkãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã™ã‚‹å ´åˆã®ã¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’ä½œæˆ
    train_loaders = [train_loader_span]
    if len(train_bywork) > 0:
        train_loader_bywork = DataLoader(train_bywork, batch_size=bywork_batch_size, shuffle=True, **loader_args)
        train_loaders.append(train_loader_bywork)
    else:
        logger.info("â„¹ï¸ No ByWork data - using RandomSpan only")
    
    # ğŸ†• val_loaderã«ã‚‚collatorã‚’é©ç”¨
    val_loader = DataLoader(val_dataset, batch_size=actual_batch_size * 2, shuffle=False, collate_fn=span_collator, **loader_args)
    
    return train_loaders, val_loader, dataset


def create_optimizer_and_scheduler(model, config: TrainingConfig, train_loaders):
    """ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã¨ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã®ä½œæˆ"""
    optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)
    
    total_steps_per_epoch = sum(len(loader) for loader in train_loaders)
    total_steps = total_steps_per_epoch * config.epochs // config.accumulation_steps

    if config.scheduler_type == 'onecycle':
        scheduler = OneCycleLR(
            optimizer, 
            max_lr=config.learning_rate * 10, 
            total_steps=total_steps, 
            pct_start=0.3, 
            anneal_strategy='cos', 
            div_factor=25.0, 
            final_div_factor=1e4
        )
        logger.info(f"ğŸ“ˆ OneCycleLR scheduler initialized (total_steps={total_steps})")
    elif config.scheduler_type == 'linear_warmup':
        scheduler = get_linear_schedule_with_warmup(
            optimizer, 
            num_warmup_steps=config.warmup_steps, 
            num_training_steps=total_steps
        )
        logger.info(f"ğŸ“ˆ Linear warmup scheduler initialized (warmup_steps={config.warmup_steps}, total_steps={total_steps})")
    else:
        scheduler = None
        
    return optimizer, scheduler


def train_epoch(model, loaders, optimizer, scheduler, scaler, device, config: TrainingConfig, epoch: int):
    """1ã‚¨ãƒãƒƒã‚¯åˆ†ã®å­¦ç¿’å‡¦ç†"""
    model.train()
    total_loss = 0
    use_bf16 = config.use_bfloat16 and device.type == "cuda" and torch.cuda.is_bf16_supported()

    for loader in loaders:
        pbar = tqdm(loader, desc=f"Epoch {epoch + 1}/{config.epochs}")
        for batch_idx, batch in enumerate(pbar):
            if batch_idx % config.accumulation_steps == 0:
                optimizer.zero_grad(set_to_none=True)

            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            labels = batch["labels"].to(device)
            
            loss_divisor = config.accumulation_steps
            
            # ğŸ†• æ–°ã—ã„autocastå½¢å¼ã‚’ä½¿ç”¨
            if use_bf16:
                with autocast(device_type='cuda', dtype=torch.bfloat16):
                    outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
                    loss = outputs.loss / loss_divisor
            else:
                with autocast(enabled=False):
                    outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
                    loss = outputs.loss / loss_divisor

            if scaler:
                scaler.scale(loss).backward()
            else:
                loss.backward()

            if (batch_idx + 1) % config.accumulation_steps == 0:
                if config.gradient_clip > 0:
                    if scaler:
                        scaler.unscale_(optimizer)
                    torch.nn.utils.clip_grad_norm_(model.parameters(), config.gradient_clip)
                
                if scaler:
                    scaler.step(optimizer)
                    scaler.update()
                else:
                    optimizer.step()
                
                if scheduler:
                    scheduler.step()

            batch_loss = loss.item() * loss_divisor
            total_loss += batch_loss
            pbar.set_postfix(loss=f"{batch_loss:.4f}", lr=f"{optimizer.param_groups[0]['lr']:.2e}")
            
    return total_loss


def train_model(config: TrainingConfig):
    """æœ€é©åŒ–ã•ã‚ŒãŸå­¦ç¿’é–¢æ•°(ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°ç‰ˆ)"""
    device, tokenizer, model = setup_training(config)
    train_loaders, val_loader, dataset = create_dataloaders(config, tokenizer)
    optimizer, scheduler = create_optimizer_and_scheduler(model, config, train_loaders)
    
    use_bf16 = config.use_bfloat16 and device.type == "cuda" and torch.cuda.is_bf16_supported()
    if config.use_bfloat16 and not use_bf16:
        logger.warning("âš ï¸ BFloat16 requested but not supported. Falling back to FP16.")
    
    scaler = GradScaler() if config.use_amp and device.type == "cuda" and not use_bf16 else None
    
    early_stopping = EarlyStopping(patience=config.patience)
    best_val_loss = float('inf')
    os.makedirs(config.save_dir, exist_ok=True)
    
    freeze_encoder_layers(model, ratio=0.5)
    logger.info("ğŸ”’ Encoder layers partially frozen (ratio=0.5)")

    for epoch in range(config.epochs):
        start_prob, end_prob = 0.5, 0.1
        current_prob = start_prob + (end_prob - start_prob) * (epoch / max(1, config.epochs - 1))
        
        # RandomSpanãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«multi_probã‚’è¨­å®š
        for ds in dataset.datasets:
            if isinstance(ds, TranslationDatasetRandomSpan):
                ds.set_multi_prob(current_prob)
        logger.info(f"ğŸ“‰ RandomSpan multi_prob = {current_prob:.2f}")

        train_loss = train_epoch(model, train_loaders, optimizer, scheduler, scaler, device, config, epoch)
        
        if epoch == 1:
            for p in model.parameters():
                p.requires_grad = True
            logger.info("ğŸ”“ Encoder fully unfrozen")

        val_loss = evaluate_model(model, val_loader, device)
        total_train_samples = sum(len(l.dataset) for l in train_loaders)
        avg_train_loss = train_loss / total_train_samples if total_train_samples > 0 else 0
        logger.info(f"ğŸ“Š Epoch {epoch+1}/{config.epochs} -> Train loss: {avg_train_loss:.4f}, Validation loss: {val_loss:.4f}")
        
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            save_path = os.path.join(config.save_dir, "best_model")
            model.save_pretrained(save_path)
            tokenizer.save_pretrained(save_path)
            logger.info(f"â­ New best model saved to {save_path}!")
        
        early_stopping(val_loss)
        if early_stopping.early_stop:
            logger.info(f"ğŸ›‘ Early stopping triggered at epoch {epoch+1}")
            break
    
    logger.info(f"\nâœ… Training completed! Best validation loss: {best_val_loss:.4f}")
    return model, tokenizer


# ===============================
# 6. ç¿»è¨³é–¢æ•°
# ===============================
def translate(model, tokenizer, text, max_length=64, num_beams=4):
    if hasattr(tokenizer, 'supported_language_codes'):
        text = ">>jap<< " + text
    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=max_length)
    device = next(model.parameters()).device
    inputs = {k: v.to(device) for k, v in inputs.items()}
    with torch.no_grad():
        outputs = model.generate(**inputs, max_length=max_length, num_beams=num_beams)
    return tokenizer.decode(outputs[0], skip_special_tokens=True)


def batch_translate(model, tokenizer, texts, batch_size=8, max_length=64, num_beams=4):
    device = next(model.parameters()).device
    if hasattr(tokenizer, 'supported_language_codes'):
        texts = [">>jap<< " + t for t in texts]
    translations = []
    for i in range(0, len(texts), batch_size):
        batch_texts = texts[i:i + batch_size]
        inputs = tokenizer(batch_texts, return_tensors="pt", padding=True, truncation=True, max_length=max_length)
        inputs = {k: v.to(device) for k, v in inputs.items()}
        with torch.no_grad():
            outputs = model.generate(**inputs, max_length=max_length, num_beams=num_beams)
        translations.extend([tokenizer.decode(o, skip_special_tokens=True) for o in outputs])
    return translations


# ===============================
# ğŸ†• 7. ãƒ¢ãƒƒã‚¯å®Ÿè¡Œç”¨ã®ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°
# ===============================
def quick_mock_test():
    """æ§‹æ–‡ãƒã‚§ãƒƒã‚¯ç”¨ã®ã‚¯ã‚¤ãƒƒã‚¯ãƒ†ã‚¹ãƒˆ"""
    logger.info("\n" + "="*60)
    logger.info("ğŸ§ª QUICK MOCK TEST - Syntax and Basic Functionality Check")
    logger.info("="*60 + "\n")
    
    config = TrainingConfig(
        model_name="Helsinki-NLP/opus-mt-en-jap",
        file_paths=[],  # ãƒ¢ãƒƒã‚¯ãƒ¢ãƒ¼ãƒ‰ã§ã¯ä¸è¦
        epochs=1,
        batch_size=4,
        mock_mode=True,
        mock_samples=20,
        mock_force_cpu=True,
        num_workers=0,
        accumulation_steps=1,
        use_amp=False,
        use_bfloat16=False,
        save_dir="./mock_output"
    )
    
    try:
        model, tokenizer = train_model(config)
        
        # ç¿»è¨³ãƒ†ã‚¹ãƒˆ
        logger.info("\nğŸ§ª Testing translation...")
        test_sentences = ["I like apples.", "How are you?"]
        results = batch_translate(model, tokenizer, test_sentences)
        
        for en, ja in zip(test_sentences, results):
            logger.info(f"  EN: {en} -> JA: {ja}")
        
        logger.info("\nâœ… MOCK TEST PASSED - All syntax checks successful!")
        return True
        
    except Exception as e:
        logger.error(f"\nâŒ MOCK TEST FAILED: {e}")
        import traceback
        traceback.print_exc()
        return False


# ===============================
# å®Ÿè¡Œä¾‹
# ===============================
if __name__ == "__main__":
    # ğŸ†• ç’°å¢ƒå¤‰æ•°ã§ãƒ¢ãƒ¼ãƒ‰ã‚’åˆ‡ã‚Šæ›¿ãˆ
    import sys
    
    if "--mock" in sys.argv or os.getenv("MOCK_MODE") == "1":
        # ãƒ¢ãƒƒã‚¯ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œ
        quick_mock_test()
    
    elif "--mock-with-files" in sys.argv:
        # ãƒ¢ãƒƒã‚¯ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆã—ã¦å®Ÿè¡Œ
        logger.info("ğŸ­ Creating mock JSONL files...")
        mock_files = create_mock_jsonl_files(output_dir="./mock_data", num_files=2, samples_per_file=50)
        
        config = TrainingConfig(
            model_name="Helsinki-NLP/opus-mt-en-jap",
            file_paths=mock_files,
            epochs=1,
            batch_size=4,
            mock_mode=True,
            mock_force_cpu=True,
            save_dir="./mock_output"
        )
        
        model, tokenizer = train_model(config)
    
    else:
        # é€šå¸¸ãƒ¢ãƒ¼ãƒ‰ï¼ˆå®Ÿéš›ã®å­¦ç¿’ï¼‰
        files = [
            "./../data/sepalated_dataset.jsonl",
            "./../data/OpenSubtitles_sample_40000.jsonl",
            "./../data/TED_sample_40000.jsonl",
            "./../data/Tatoeba_sample_40000.jsonl",
            "./../data/all_outenjp.jsonl"
        ]
        
        # å„ãƒ•ã‚¡ã‚¤ãƒ«ã«å¯¾å¿œã™ã‚‹ã‚¿ã‚° (å¿…è¦ã«å¿œã˜ã¦è¨­å®š)
        # tags = [None, None, None, None, "[LYRICS]"]
        tags = None
       
        config = TrainingConfig(
            model_name="Helsinki-NLP/opus-mt-en-jap",
            file_paths=files,
            epochs=2,
            batch_size=16,
            max_samples_per_span_file=40000,
            save_dir="./models/translation_model_final",
            random_seed=42,
            tags=tags,
            num_workers=4,
            accumulation_steps=4,
            use_bfloat16=True,
            scheduler_type='onecycle'
        )
        
        model, tokenizer = train_model(config)
        
        # ç¿»è¨³ãƒ†ã‚¹ãƒˆ
        test_sentences = [
            "I like apples.",
            "How are you?",
            "Machine learning is fun.",
            "I couldn't speak English well."
        ]
        results = batch_translate(model, tokenizer, test_sentences)
        for en, ja in zip(test_sentences, results):
            print(f"EN: {en} -> JA: {ja}")