import torch
from tqdm import tqdm
import logging
import matplotlib.pyplot as plt
import matplotlib
import random

# ãƒ˜ãƒƒãƒ‰ãƒ¬ã‚¹ç’°å¢ƒï¼ˆã‚µãƒ¼ãƒãƒ¼ç­‰ï¼‰ã§ã®ãƒ—ãƒ­ãƒƒãƒˆã‚¨ãƒ©ãƒ¼é˜²æ­¢
matplotlib.use('Agg')

logger = logging.getLogger(__name__)

# ===============================
# 1. LRFinder (ãƒ¡ãƒ¢ãƒªç¯€ç´„ç‰ˆ)
# ===============================
class LRFinder:
    """æœ€é©ãªå­¦ç¿’ç‡ã‚’è‡ªå‹•æ¢ç´¢ (CPUãƒ¡ãƒ¢ãƒªã«é…æ…®)"""
    def __init__(self, model, optimizer, device):
        self.model = model
        self.optimizer = optimizer
        self.device = device
        self.history = {'lr': [], 'loss': []}
        
    def find(self, train_loader, min_lr=1e-7, max_lr=1, num_iter=100, smooth_f=0.05):
        logger.info(f"ğŸ” LR Finder: Searching ({min_lr} to {max_lr})...")
        
        # âš ï¸ deepcopyã‚’é¿ã‘ã€é‡ã¿ã ã‘ã‚’CPUã«ã‚¯ãƒ­ãƒ¼ãƒ³ã—ã¦ä¿å­˜ (ãƒ¡ãƒ¢ãƒªç¯€ç´„)
        original_weights = {k: v.cpu().clone() for k, v in self.model.state_dict().items()}
        original_opt_state = {k: v for k, v in self.optimizer.state_dict().items()}
        
        mult = (max_lr / min_lr) ** (1 / num_iter)
        lr = min_lr
        self.optimizer.param_groups[0]['lr'] = lr
        
        avg_loss = 0.0
        best_loss = float('inf')
        
        self.model.train()
        iterator = iter(train_loader)
        
        for i in range(num_iter):
            try:
                batch = next(iterator)
            except StopIteration:
                iterator = iter(train_loader)
                batch = next(iterator)
            
            self.optimizer.zero_grad()
            input_ids = batch['input_ids'].to(self.device)
            attention_mask = batch['attention_mask'].to(self.device)
            labels = batch['labels'].to(self.device)
            
            outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
            loss = outputs.loss
            
            # æå¤±ã®å¹³æ»‘åŒ–
            avg_loss = smooth_f * loss.item() + (1 - smooth_f) * avg_loss if i > 0 else loss.item()
            
            if avg_loss < best_loss: best_loss = avg_loss
            if i > 1 and avg_loss > 4 * best_loss:
                logger.warning("âš ï¸ Loss diverged, stopping LR finder.")
                break
                
            self.history['lr'].append(lr)
            self.history['loss'].append(avg_loss)
            
            loss.backward()
            self.optimizer.step()
            
            lr *= mult
            for pg in self.optimizer.param_groups: pg['lr'] = lr
            
        # çŠ¶æ…‹ã®å¾©å…ƒ
        self.model.load_state_dict(original_weights)
        self.optimizer.load_state_dict(original_opt_state)
        
        if self.history['loss']:
            suggested_lr = self.history['lr'][self.history['loss'].index(min(self.history['loss']))] // 10
        else:
            suggested_lr = min_lr
            
        logger.info(f"âœ… Suggested LR: {suggested_lr:.2e}")
        return suggested_lr

    def plot(self, save_path):
        plt.figure(figsize=(10, 6))
        plt.plot(self.history['lr'], self.history['loss'])
        plt.xscale('log')
        plt.xlabel('Learning Rate')
        plt.ylabel('Loss')
        plt.grid(True)
        plt.savefig(save_path)
        plt.close()

# ===============================
# 2. å­¦ç¿’è£œåŠ©
# ===============================
class EarlyStopping:
    def __init__(self, patience=2):
        self.patience = patience
        self.counter = 0
        self.best_loss = float('inf')
        self.early_stop = False

    def __call__(self, val_loss):
        if val_loss < self.best_loss:
            self.best_loss = val_loss
            self.counter = 0
        else:
            self.counter += 1
            if self.counter >= self.patience: self.early_stop = True

# ã‚¤ãƒ³ã‚¿ãƒ¼ãƒªãƒ¼ãƒ–å­¦ç¿’
class InterleavedLoaders:
    """
    è¤‡æ•°ã®DataLoaderã‚’ãƒãƒƒãƒå˜ä½ã§æ··ãœåˆã‚ã›ã‚‹ã€‚
    weightsã‚’æŒ‡å®šã™ã‚‹ã“ã¨ã§ã€ç‰¹å®šã®ãƒ‡ãƒ¼ã‚¿ã®å‡ºç¾é »åº¦ã‚’èª¿æ•´å¯èƒ½ã€‚
    """
    def __init__(self, loaders_dict, weights=None):
        # Noneã‚’é™¤å¤–ã—ã¦æœ‰åŠ¹ãªãƒ­ãƒ¼ãƒ€ãƒ¼ã ã‘ã‚’ä¿æŒ
        self.loaders = {k: v for k, v in loaders_dict.items() if v is not None}
        self.keys = list(self.loaders.keys())
        self.weights = weights if weights else [1.0] * len(self.keys)
        
    def __iter__(self):
        # å„ãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚¿åŒ–
        iters = {k: iter(v) for k, v in self.loaders.items()}
        finished = set()

        while len(finished) < len(self.loaders):
            # é‡ã¿ã«åŸºã¥ã„ã¦æ¬¡ã«ä½¿ã†ãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’é¸æŠ
            active_keys = [k for k in self.keys if k not in finished]
            if not active_keys: break
            
            # ç¢ºç‡çš„ã«é¸æŠ
            curr_key = random.choices(
                active_keys, 
                weights=[self.weights[self.keys.index(k)] for k in active_keys]
            )[0]
            
            try:
                yield next(iters[curr_key])
            except StopIteration:
                finished.add(curr_key)

    def __len__(self):
        # å…¨ãƒ­ãƒ¼ãƒ€ãƒ¼ã®ãƒãƒƒãƒæ•°ã®åˆè¨ˆ
        return sum(len(v) for v in self.loaders.values())

def get_phase_loaders(phase_idx, loaders_map):
    """
    ãƒ•ã‚§ãƒ¼ã‚ºã”ã¨ã®ãƒ­ãƒ¼ãƒ€ãƒ¼ãƒªã‚¹ãƒˆï¼ˆã¾ãŸã¯ã‚¤ãƒ³ã‚¿ãƒ¼ãƒªãƒ¼ãƒ–ï¼‰ã‚’è¿”ã™ã€‚
    """
    if phase_idx == 0:
        # Phase 1: åŸºç¤
        return [loaders_map["span"], loaders_map["bywork"]]
    elif phase_idx == 1:
        # Phase 2: æ–‡è„ˆ
        return [loaders_map["chunk"], loaders_map["bywork"], loaders_map["span"]]
    else:
        # Phase 3: æœ¬å‘½ï¼ˆã‚¤ãƒ³ã‚¿ãƒ¼ãƒªãƒ¼ãƒ–ï¼‰
        p3_dict = {
            "pc": loaders_map["practical_chunk"],
            "pl": loaders_map["practical_line"],
            "anchor": loaders_map["span"]
        }
        return [InterleavedLoaders(p3_dict, weights=[1.0, 1.0, 2.0])]

def get_total_steps(phase_epochs, loaders_map, config):
    total_updates = 0
    for phase_idx, n_epochs in enumerate(phase_epochs):
        if n_epochs <= 0: continue
        
        loaders = get_phase_loaders(phase_idx, loaders_map)
        
        phase_it = sum(len(l) for l in loaders if l is not None)
        total_updates += (phase_it // config.accumulation_steps) * n_epochs
    return total_updates

# ===============================
# 3. å­¦ç¿’ã‚³ã‚¢
# ===============================
def train_epoch(model, loaders, optimizer, scheduler, scaler, device, config, epoch, criterion=None, ema=None):
    model.train()
    total_loss = 0
    update_count = 0
    if not isinstance(loaders, list):
        loaders = [loaders]
        
    # è¤‡æ•°ã®ãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’é †ç•ªã«å›ã™
    for loader in loaders:
        if loader is None: continue
        pbar = tqdm(loader, desc=f"Epoch {epoch+1}")
        
        for batch_idx, batch in enumerate(pbar):
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            labels = batch["labels"].to(device)

            with torch.cuda.amp.autocast(enabled=config.use_amp):
                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
                loss = (criterion(outputs.logits, labels) if criterion else outputs.loss) / config.accumulation_steps

            if scaler:
                scaler.scale(loss).backward()
            else:
                loss.backward()

            if (batch_idx + 1) % config.accumulation_steps == 0:
                if scaler:
                    scaler.unscale_(optimizer)
                    torch.nn.utils.clip_grad_norm_(model.parameters(), config.gradient_clip)
                    scaler.step(optimizer)
                    scaler.update()
                else:
                    torch.nn.utils.clip_grad_norm_(model.parameters(), config.gradient_clip)
                    optimizer.step()
                
                optimizer.zero_grad(set_to_none=True)
                if ema: ema.update()
                
                # Optimizerã®æ›´æ–°å¾Œã«Schedulerã‚’é€²ã‚ã‚‹ï¼ˆè­¦å‘Šå¯¾ç­–ï¼‰
                if scheduler: scheduler.step()
                update_count += 1

            total_loss += loss.item() * config.accumulation_steps
            pbar.set_postfix(lr=f"{optimizer.param_groups[0]['lr']:.1e}", loss=f"{loss.item()*config.accumulation_steps:.4f}")

    return total_loss / update_count if update_count > 0 else total_loss


def evaluate_model(model, val_loader, device, config, criterion=None, ema=None):
    """æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã§ã®æå¤±ã‚’è¨ˆç®—"""
    model.eval()
    if ema: ema.apply_shadow() # EMAã‚’é©ç”¨ã—ã¦è©•ä¾¡
    
    total_loss = 0
    # è©•ä¾¡æ™‚ã¯å‹¾é…è¨ˆç®—ã‚’ã‚ªãƒ•ã«ã—ã¦ãƒ¡ãƒ¢ãƒªã‚’ç¯€ç´„
    with torch.no_grad():
        for batch in val_loader:
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            labels = batch["labels"].to(device)
            
            # ä¿®æ­£: torch.amp ã§ã¯ãªã torch.cuda.amp ã‚’ä½¿ç”¨ï¼ˆJetsonäº’æ›æ€§ï¼‰
            with torch.cuda.amp.autocast(enabled=config.use_amp and device.type == 'cuda'):
                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
                loss = (criterion(outputs.logits, labels) if criterion else outputs.loss)
            
            total_loss += loss.item()
            
    if ema: ema.restore() # å­¦ç¿’ç”¨ã«é‡ã¿ã‚’æˆ»ã™
    model.train()
    return total_loss / len(val_loader) if len(val_loader) > 0 else 0