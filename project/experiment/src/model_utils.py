import torch
import torch.nn as nn
import torch.nn.functional as F
import logging
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from transformers import MarianTokenizer
logger = logging.getLogger(__name__)

# ===============================
# 1. æå¤±é–¢æ•°: Focal Loss
# ===============================
class FocalLoss(nn.Module):
    """
    é›£ã—ã„ã‚µãƒ³ãƒ—ãƒ«ï¼ˆæ­£è§£ç¢ºç‡ãŒä½ã„ã‚‚ã®ï¼‰ã«é«˜ã„é‡ã¿ã‚’ç½®ãæå¤±é–¢æ•°
    å…¬å¼: FL(p_t) = -alpha * (1 - p_t)^gamma * log(p_t)
    """
    def __init__(self, alpha=0.25, gamma=2.0, reduction='mean'):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction
        
    def forward(self, logits, targets, ignore_index=-100):
        logits_flat = logits.view(-1, logits.size(-1))
        targets_flat = targets.view(-1)
        
        mask = targets_flat != ignore_index
        valid_logits = logits_flat[mask]
        valid_targets = targets_flat[mask]
        
        if valid_logits.size(0) == 0:
            return torch.tensor(0.0, device=logits.device, requires_grad=True)
        
        ce_loss = F.cross_entropy(valid_logits, valid_targets, reduction='none')
        p_t = torch.exp(-ce_loss)
        focal_loss = self.alpha * (1 - p_t) ** self.gamma * ce_loss
        
        if self.reduction == 'mean':
            return focal_loss.mean()
        elif self.reduction == 'sum':
            return focal_loss.sum()
        return focal_loss

# ===============================
# 2. æ­£å‰‡åŒ–: EMA (Exponential Moving Average)
# ===============================
class EMA:
    """
    ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æŒ‡æ•°ç§»å‹•å¹³å‡ã‚’ä¿æŒã—ã€æ¨è«–æ™‚ã®å®‰å®šæ€§ã‚’å‘ä¸Šã•ã›ã‚‹
    """
    def __init__(self, model, decay=0.9999):
        self.model = model
        self.decay = decay
        self.shadow = {}
        self.backup = {}
        
    def register(self):
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                self.shadow[name] = param.data.clone()
        logger.info(f"âœ… EMA registered (decay={self.decay})")
    
    def update(self):
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                new_average = (1.0 - self.decay) * param.data + self.decay * self.shadow[name]
                self.shadow[name] = new_average.clone()
    
    def apply_shadow(self):
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                self.backup[name] = param.data.clone()
                param.data = self.shadow[name]
    
    def restore(self):
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                param.data = self.backup[name]
        self.backup = {}

# ===============================
# 3. ãƒ¢ãƒ‡ãƒ«åˆ¶å¾¡ãƒ»åˆæœŸåŒ–
# ===============================

def freeze_encoder_layers(model, ratio=0.5):
    """
    Encoderã®ä¸‹ä½ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’å‡çµã—ã€äº‹å‰å­¦ç¿’æ¸ˆã¿ã®çŸ¥è­˜ã‚’ä¿è­·ã™ã‚‹
    """
    encoder = getattr(model.get_encoder(), 'layers', None)
    if encoder is None:
        logger.warning("âš ï¸ Could not find encoder layers to freeze.")
        return

    total_layers = len(encoder)
    freeze_count = int(total_layers * ratio)
    for i, layer in enumerate(encoder):
        if i < freeze_count:
            for param in layer.parameters():
                param.requires_grad = False
    logger.info(f"ğŸ”’ Frozen {freeze_count}/{total_layers} encoder layers (ratio={ratio})")

def setup_model_and_tokenizer(config, device):
    """
    ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã€åˆæœŸè¨­å®šã‚’è¡Œã†
    """
    try:
    # 1. ã¾ãšã¯Marianå°‚ç”¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã§ãƒ­ãƒ¼ãƒ‰ã‚’è©¦ã¿ã‚‹
        tokenizer = MarianTokenizer.from_pretrained(config.model_name)
    except Exception:
    # 2. ãƒ€ãƒ¡ãªã‚‰AutoTokenizerã«æˆ»ã™ï¼ˆä¿é™ºï¼‰
        logger.warning("âš ï¸ Failed to load MarianTokenizer, falling back to AutoTokenizer.")
        tokenizer = AutoTokenizer.from_pretrained(config.model_name, 
    use_fast=False,   # sentencepieceã‚’ç¢ºå®Ÿã«ä½¿ã†ãŸã‚ã€ã‚ãˆã¦Falseã«
    trust_remote_code=True)

# é‡è¦ï¼šæ­£ã—ã„è¨€èªã‚³ãƒ¼ãƒ‰ã‚’æ•™ãˆã‚‹ï¼ˆMarianMTã¯ã“ã“ãŒè‚ï¼ï¼‰
# opus-mt-en-jap ã®å ´åˆã€ã‚½ãƒ¼ã‚¹ã¯ 'en', ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã¯ 'ja' (ã¾ãŸã¯ 'jpn') ã ãŒã€
# MarianTokenizerã¯è‡ªå‹•åˆ¤å®šã—ã¦ãã‚Œã‚‹ã“ã¨ãŒå¤šã„ã€‚å¿µã®ãŸã‚ç¢ºèªãƒ­ã‚°ã‚’å‡ºã™ã€‚
    logger.info(f"ğŸ§© Tokenizer Vocab Size: {tokenizer.vocab_size}")
    
    
    # FP32ã§ãƒ­ãƒ¼ãƒ‰ (GradScaler/AMPã§å‹•çš„ã«åˆ¶å¾¡ã™ã‚‹ãŸã‚)
    model = AutoModelForSeq2SeqLM.from_pretrained(
        config.model_name,
        use_safetensors=True
    ).to(device)
    
    # Label Smoothingã®è¨­å®š
    if config.use_label_smoothing and hasattr(model.config, 'label_smoothing'):
        model.config.label_smoothing = config.label_smoothing
        logger.info(f"âœ¨ Label Smoothing enabled: {config.label_smoothing}")
    
    # torch.compile (åˆ©ç”¨å¯èƒ½ãªå ´åˆã®ã¿)
    if config.use_compile and hasattr(torch, 'compile'):
        try:
            model = torch.compile(model)
            logger.info("âš¡ Model compiled with torch.compile")
        except Exception as e:
            logger.warning(f"âš ï¸ torch.compile skipped: {e}")
            
    return model, tokenizer