import json
import random
import logging
import torch
from torch.utils.data import Dataset, DataLoader, ConcatDataset, Subset, random_split
from tqdm import tqdm
from transformers import DataCollatorForSeq2Seq
import os
logger = logging.getLogger(__name__)

# ===============================
# 1. Dataset ã‚¯ãƒ©ã‚¹ç¾¤
# ===============================

class TranslationDatasetBase(Dataset):
    def __init__(self, en_texts, ja_texts, tokenizer, max_len=64):
        self.en_texts = en_texts
        self.ja_texts = ja_texts
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.en_texts)

    def _get_tokenized_pair(self, en, ja):
        # ç‰¹å®šã®ãƒ¢ãƒ‡ãƒ«ç”¨ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ï¼ˆHelsinki-NLP / mBARTç­‰ï¼‰
        if hasattr(self.tokenizer, 'supported_language_codes'):
            en = ">>jap<< " + en
        
        inputs = self.tokenizer(en, max_length=self.max_len, truncation=True, padding=False)
        labels = self.tokenizer(ja, max_length=self.max_len, truncation=True, padding=False)
        return {
            "input_ids": inputs["input_ids"],
            "attention_mask": inputs["attention_mask"],
            "labels": labels["input_ids"]
        }

class TranslationDatasetRandomSpan(TranslationDatasetBase):
    def __init__(self, en_texts, ja_texts, tokenizer, max_len=64, multi_prob=0.5):
        super().__init__(en_texts, ja_texts, tokenizer, max_len)
        self.multi_prob = multi_prob

    def __getitem__(self, idx):
        en, ja = self.en_texts[idx], self.ja_texts[idx]
        # ãƒãƒ«ãƒã‚»ãƒ³ãƒ†ãƒ³ã‚¹åŒ–ï¼ˆæ–‡è„ˆã‚’æŒãŸã›ã‚‹ï¼‰
        if random.random() < self.multi_prob and idx + 1 < len(self.en_texts):
            en = f"{en} {self.en_texts[idx+1]}"
            ja = f"{ja} {self.ja_texts[idx+1]}"
        return self._get_tokenized_pair(en, ja)

class TranslationDatasetByWork(TranslationDatasetBase):
    def __getitem__(self, idx):
        return self._get_tokenized_pair(self.en_texts[idx], self.ja_texts[idx])

# ===============================
# 2. ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰ãƒ»ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# ===============================

def is_chunk_delimiter(text):
    return any(d in text for d in ["%%%%%%%%THISWORKENDSHERE%%%%%%%%", "%%%%%%%%ã“ã®ä½œå“ã“ã“ã¾ã§%%%%%%%%"])

# data_utils.py (ä¿®æ­£ç‰ˆ)

def generate_mock_data(num_samples=50):
    """å®Œå…¨ã«ãƒ©ãƒ³ãƒ€ãƒ ãªç¿»è¨³ãƒšã‚¢ã‚’ç”Ÿæˆ"""
    en_samples = [f"This is mock english sentence {i}." for i in range(num_samples)]
    ja_samples = [f"ã“ã‚Œã¯ãƒ¢ãƒƒã‚¯ã®æ—¥æœ¬èªæ–‡ç«  {i} ã§ã™ã€‚" for i in range(num_samples)]
    return en_samples, ja_samples




def load_jsonl(file_path, tag=None, max_samples=None):
    en_list, ja_list = [], []
    logger.info(f"ğŸ“– Reading: {file_path}")
    
    with open(file_path, "r", encoding="utf-8") as f:
        lines = f.readlines()
        if max_samples and len(lines) > max_samples:
            lines = random.sample(lines, max_samples)
            
        for line in lines:
            try:
                data = json.loads(line)
                en, ja = data.get("en"), data.get("ja")
                if en and ja and not is_chunk_delimiter(en):
                    en_list.append(f"{tag} {en}" if tag else en)
                    ja_list.append(ja)
            except: continue
    return en_list, ja_list

def load_chunks(file_path, tag=None):
    chunks_en, chunks_ja = [], []
    curr_en, curr_ja = [], []
    
    with open(file_path, "r", encoding="utf-8") as f:
        for line in f:
            try:
                data = json.loads(line)
                en, ja = data.get("en"), data.get("ja")
                if is_chunk_delimiter(en):
                    if curr_en:
                        chunks_en.append(f"{tag} {' '.join(curr_en)}" if tag else ' '.join(curr_en))
                        chunks_ja.append(' '.join(curr_ja))
                        curr_en, curr_ja = [], []
                elif en and ja:
                    curr_en.append(en); curr_ja.append(ja)
            except: continue

    # âœ¨ ãƒ«ãƒ¼ãƒ—çµ‚äº†å¾Œã€æ®‹ã£ã¦ã„ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Œã°è¿½åŠ ã™ã‚‹
    if curr_en:
        chunks_en.append(f"{tag} {' '.join(curr_en)}" if tag else ' '.join(curr_en))
        chunks_ja.append(' '.join(curr_ja))

    return chunks_en, chunks_ja

# ===============================
# 3. DataLoader ç”Ÿæˆ (3-Phaseå¯¾å¿œ)
# ===============================

def create_dataloaders(config, tokenizer):
    """ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€ãƒ•ã‚§ãƒ¼ã‚ºã”ã¨ã®Loaderã‚’ãƒãƒƒãƒ”ãƒ³ã‚°ã—ã¦è¿”ã™"""
    if config.mock_mode:
        # ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        first_file = config.file_paths[0] if config.file_paths else None
        
        if first_file and os.path.exists(first_file):
            logger.info(f"ğŸ­ LIGHT MOCK: Sampling 20 real lines from {first_file}...")
            # æœ€åˆã®ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰20è¡Œã ã‘æœ¬ç‰©ã‚’å€Ÿã‚Šã¦ãã‚‹
            en, ja = load_jsonl(first_file, max_samples=20)
        else:
            logger.info("ğŸ­ DUMMY MOCK: No files found, using synthetic data...")
            en = ["I love you.", "Who are you?", "This is a pen."] * 7
            ja = ["ç§ã¯ã‚ãªãŸã‚’æ„›ã—ã¦ã„ã¾ã™ã€‚", "ã‚ãªãŸã¯èª°ã§ã™ã‹ï¼Ÿ", "ã“ã‚Œã¯ãƒšãƒ³ã§ã™ã€‚"] * 7
        
        mock_ds = TranslationDatasetRandomSpan(en, ja, tokenizer, max_len=config.max_len)
        loader = DataLoader(mock_ds, batch_size=4, shuffle=True, 
                          collate_fn=DataCollatorForSeq2Seq(tokenizer, padding=True))
        
        return {k: loader for k in ["span", "bywork", "chunk", "practical_line", "practical_chunk", "val"]}
    
    loaders_map = {"span": None, "bywork": None, "chunk": None, "practical_line": None, "practical_chunk": None}
    all_val_datasets = []

    for path, ftype in zip(config.file_paths, config.file_types):
        # 0: Span, 1: ByWork, 2: Practical
        if ftype == 0:
            en, ja = load_jsonl(path, max_samples=config.max_samples_per_span_file)
            ds = TranslationDatasetRandomSpan(en, ja, tokenizer, max_len=config.max_len)
            loaders_map["span"] = ds # å¾Œã§LoaderåŒ–
        
        elif ftype == 1:
            en, ja = load_jsonl(path)
            loaders_map["bywork"] = TranslationDatasetByWork(en, ja, tokenizer, config.max_len)
            cen, cja = load_chunks(path)
            loaders_map["chunk"] = TranslationDatasetByWork(cen, cja, tokenizer, config.max_len * 4)

        elif ftype == 2:
            en, ja = load_jsonl(path)
            # 20å€ã¯å¤šã™ããŸã®ã§ã€configã§åˆ¶å¾¡å¯èƒ½ã«ã™ã‚‹ã‹5å€ç¨‹åº¦ã«æŠ‘ãˆã‚‹
            factor = getattr(config, 'practical_upsample', 2)
            loaders_map["practical_line"] = ConcatDataset([TranslationDatasetByWork(en, ja, tokenizer, config.max_len)] * factor)
            
            cen, cja = load_chunks(path)
            loaders_map["practical_chunk"] = ConcatDataset([TranslationDatasetByWork(cen, cja, tokenizer, config.max_len * 4)] * factor)

    # å„ Dataset ã‚’ DataLoader ã«å¤‰æ› (ç°¡æ˜“åŒ–ã®ãŸã‚ã“ã“ã§ã¯ä¸€æ‹¬å‡¦ç†)
    collator = DataCollatorForSeq2Seq(tokenizer, padding=True, label_pad_token_id=-100)
    
    final_loaders = {}
    for key, ds in loaders_map.items():
        if ds:
            # ã“ã“ã§ Train/Val åˆ†å‰²ã‚’å…¥ã‚Œã‚‹ã®ãŒç†æƒ³çš„
            final_loaders[key] = DataLoader(ds, batch_size=config.batch_size, shuffle=True, collate_fn=collator)

    return final_loaders