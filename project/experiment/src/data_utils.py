import json
import random
import logging
import torch
from torch.utils.data import Dataset, DataLoader, ConcatDataset, Subset, random_split
from tqdm import tqdm
from transformers import DataCollatorForSeq2Seq
import os
logger = logging.getLogger(__name__)

# ===============================
# 1. Dataset ã‚¯ãƒ©ã‚¹ç¾¤
# ===============================

class TranslationDatasetBase(Dataset):
    def __init__(self, en_texts, ja_texts, tokenizer, max_len=64):
        self.en_texts = en_texts
        self.ja_texts = ja_texts
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.en_texts)

    def _get_tokenized_pair(self, en, ja):
        # ç‰¹å®šã®ãƒ¢ãƒ‡ãƒ«ç”¨ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ï¼ˆHelsinki-NLP / mBARTç­‰ï¼‰
        if hasattr(self.tokenizer, 'supported_language_codes'):
            en =  en
        
        inputs = self.tokenizer(en, max_length=self.max_len, truncation=True, padding=False)
        labels = self.tokenizer(ja, max_length=self.max_len, truncation=True, padding=False)
        # === ğŸš‘ ç·Šæ€¥ãƒ‡ãƒãƒƒã‚°ç”¨ã‚³ãƒ¼ãƒ‰ (ã“ã“ã«è¿½åŠ ï¼) ===
        # 1%ã®ç¢ºç‡ã§ä¸­èº«ã‚’æš´éœ²ã™ã‚‹
        #import random
        #if random.random() < 0.01:
        #    print(f"\n[DEBUG] Raw EN: '{en}'")
        #    print(f"[DEBUG] Raw JA: '{ja}'")
        #    print(f"[DEBUG] Tokenized Labels: {labels['input_ids']}")
            # ã‚‚ã—ãƒ©ãƒ™ãƒ«ãŒ [EOS] (ä¾‹: [1] ã‚„ [2] ã ã‘) ãªã‚‰ã€ãã‚ŒãŒçŠ¯äºº
        # ==========================================
        return {
            "input_ids": inputs["input_ids"],
            "attention_mask": inputs["attention_mask"],
            "labels": labels["input_ids"]
        }

class TranslationDatasetRandomSpan(TranslationDatasetBase):
    def __init__(self, en_texts, ja_texts, tokenizer, max_len=64, multi_prob=0.5):
        super().__init__(en_texts, ja_texts, tokenizer, max_len)
        self.multi_prob = multi_prob

    def __getitem__(self, idx):
        en, ja = self.en_texts[idx], self.ja_texts[idx]
        # ãƒãƒ«ãƒã‚»ãƒ³ãƒ†ãƒ³ã‚¹åŒ–ï¼ˆæ–‡è„ˆã‚’æŒãŸã›ã‚‹ï¼‰
        if random.random() < self.multi_prob and idx + 1 < len(self.en_texts):
            en = f"{en} {self.en_texts[idx+1]}"
            ja = f"{ja} {self.ja_texts[idx+1]}"
        return self._get_tokenized_pair(en, ja)

class TranslationDatasetByWork(TranslationDatasetBase):
    def __getitem__(self, idx):
        return self._get_tokenized_pair(self.en_texts[idx], self.ja_texts[idx])

# ===============================
# 2. ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰ãƒ»ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# ===============================

def is_chunk_delimiter(text):
    return any(d in text for d in ["%%%%%%%%THISWORKENDSHERE%%%%%%%%", "%%%%%%%%ã“ã®ä½œå“ã“ã“ã¾ã§%%%%%%%%"])

# data_utils.py (ä¿®æ­£ç‰ˆ)

def generate_mock_data(num_samples=50):
    """å®Œå…¨ã«ãƒ©ãƒ³ãƒ€ãƒ ãªç¿»è¨³ãƒšã‚¢ã‚’ç”Ÿæˆ"""
    en_samples = [f"This is mock english sentence {i}." for i in range(num_samples)]
    ja_samples = [f"ã“ã‚Œã¯ãƒ¢ãƒƒã‚¯ã®æ—¥æœ¬èªæ–‡ç«  {i} ã§ã™ã€‚" for i in range(num_samples)]
    return en_samples, ja_samples




def load_jsonl(file_path, tag=None, max_samples=None):
    en_list, ja_list = [], []
    logger.info(f"ğŸ“– Reading: {file_path}")
    
    with open(file_path, "r", encoding="utf-8") as f:
        lines = f.readlines()
        
    # max_samples ã§çµã‚Šè¾¼ã¿
    if max_samples and len(lines) > max_samples:
        lines = random.sample(lines, max_samples)
            
    # â˜… tqdm ã§ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã‚’è¡¨ç¤º (é•·ã„èª­ã¿è¾¼ã¿ã§ã‚‚å®‰å¿ƒ)
    for line in tqdm(lines, desc=f"Loading {os.path.basename(file_path)}"):
        try:
            data = json.loads(line)
            en, ja = data.get("en"), data.get("ja")
            if en and ja and not is_chunk_delimiter(en):
                en_list.append(f"{tag} {en}" if tag else en)
                ja_list.append(ja)
        except: continue
            
    return en_list, ja_list

def load_chunks(file_path, tag=None):
    chunks_en, chunks_ja = [], []
    curr_en, curr_ja = [], []
    
    logger.info(f"chunks loading: {file_path}")
    with open(file_path, "r", encoding="utf-8") as f:
        # chunkã¯è¡Œæ•°ä¸æ˜ãªã“ã¨ãŒå¤šã„ã®ã§å˜ç´”ãƒ«ãƒ¼ãƒ—ã€ã‚ã‚‹ã„ã¯ tqdm(f) ã‚‚å¯
        for line in f:
            try:
                data = json.loads(line)
                en, ja = data.get("en"), data.get("ja")
                if is_chunk_delimiter(en):
                    if curr_en:
                        chunks_en.append(f"{tag} {' '.join(curr_en)}" if tag else ' '.join(curr_en))
                        chunks_ja.append(' '.join(curr_ja))
                        curr_en, curr_ja = [], []
                elif en and ja:
                    curr_en.append(en); curr_ja.append(ja)
            except: continue

    if curr_en:
        chunks_en.append(f"{tag} {' '.join(curr_en)}" if tag else ' '.join(curr_en))
        chunks_ja.append(' '.join(curr_ja))

    return chunks_en, chunks_ja

# ===============================
# 3. DataLoader ç”Ÿæˆ (3-Phaseå¯¾å¿œ)
# ===============================

def create_dataloaders(config, tokenizer):
    """ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€Train/Valã«åˆ†å‰²ã—ã¦Loaderã‚’è¿”ã™"""
    
    # --- Mock Mode (æ—¢å­˜ã®ã¾ã¾) ---
    if config.mock_mode:
        first_file = config.file_paths[0] if config.file_paths else None
        if first_file and os.path.exists(first_file):
            logger.info(f"ğŸ­ LIGHT MOCK: Sampling 20 real lines from {first_file}...")
            en, ja = load_jsonl(first_file, max_samples=20)
        else:
            logger.info("ğŸ­ DUMMY MOCK: No files found, using synthetic data...")
            en = ["I love you.", "Who are you?", "This is a pen."] * 7
            ja = ["ç§ã¯ã‚ãªãŸã‚’æ„›ã—ã¦ã„ã¾ã™ã€‚", "ã‚ãªãŸã¯èª°ã§ã™ã‹ï¼Ÿ", "ã“ã‚Œã¯ãƒšãƒ³ã§ã™ã€‚"] * 7
        
        mock_ds = TranslationDatasetRandomSpan(en, ja, tokenizer, max_len=config.max_len)
        loader = DataLoader(mock_ds, batch_size=4, shuffle=True, 
                          collate_fn=DataCollatorForSeq2Seq(tokenizer, padding=True))
        # Mockã®å ´åˆã¯å…¨éƒ¨åŒã˜Loaderã‚’ä½¿ã„å›ã™
        return {k: loader for k in ["span", "bywork", "chunk", "practical_line", "practical_chunk", "val"]}
    
    # --- æœ¬ç•ªãƒ­ãƒ¼ãƒ‰ ---
    loaders_map = {"span": None, "bywork": None, "chunk": None, "practical_line": None, "practical_chunk": None}
    span_datasets = [] 
    
    # Datasetä½œæˆãƒ«ãƒ¼ãƒ—
    for path, ftype in zip(config.file_paths, config.file_types):
        if ftype == 0:
            en, ja = load_jsonl(path, max_samples=config.max_samples_per_span_file)
            ds = TranslationDatasetRandomSpan(en, ja, tokenizer, max_len=config.max_len)
            # â˜… ä¿®æ­£2: ä¸Šæ›¸ãã›ãšã«è¿½åŠ ã™ã‚‹
            span_datasets.append(ds)
        
        elif ftype == 1:
            # ã“ã“ã¯æ›¸ãæ›ãˆæ¸ˆã¿ï¼ˆ20000ï¼‰ã®ã¯ãš
            en, ja = load_jsonl(path, max_samples=20000) 
            loaders_map["bywork"] = TranslationDatasetByWork(en, ja, tokenizer, config.max_len)
            cen, cja = load_chunks(path)
            loaders_map["chunk"] = TranslationDatasetByWork(cen, cja, tokenizer, config.max_len * 4)

        elif ftype == 2:
            en, ja = load_jsonl(path, max_samples=350)
            factor = getattr(config, 'practical_upsample', 2)
            loaders_map["practical_line"] = ConcatDataset([TranslationDatasetByWork(en, ja, tokenizer, config.max_len)] * factor)
            
            cen, cja = load_chunks(path)
            loaders_map["practical_chunk"] = ConcatDataset([TranslationDatasetByWork(cen, cja, tokenizer, config.max_len * 4)] * factor)

    # â˜… ä¿®æ­£3: ãƒ«ãƒ¼ãƒ—ã‚’æŠœã‘ãŸã‚‰ã€è²¯ã‚ãŸspanãƒ‡ãƒ¼ã‚¿ã‚’åˆä½“ã•ã›ã‚‹
    if span_datasets:
        loaders_map["span"] = ConcatDataset(span_datasets)
        logger.info(f"ğŸ“š Combined {len(span_datasets)} span datasets into one.")
    # --- â˜… Train/Val åˆ†å‰²ã¨ DataLoader åŒ– ---
    collator = DataCollatorForSeq2Seq(tokenizer, padding=True, label_pad_token_id=-100)
    final_loaders = {}
    val_datasets = [] # ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ã“ã“ã«é›†ã‚ã‚‹

    for key, ds in loaders_map.items():
        if ds:
            # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå…¨ä½“ã®é•·ã•
            full_len = len(ds)
            # 5% ã‚’ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã«ã™ã‚‹ï¼ˆãŸã ã—æœ€ä½1å€‹ã¯ç¢ºä¿ã€ãƒ‡ãƒ¼ã‚¿ãŒå°‘ãªã™ãã‚‹å ´åˆã¯åˆ†å‰²ã—ãªã„ï¼‰
            val_len = int(full_len * 0.05)
            if val_len < 1 and full_len > 1: val_len = 1
            train_len = full_len - val_len

            if val_len > 0:
                # â˜… ã“ã“ã§ random_split ã‚’ä½¿ç”¨ï¼
                train_ds, val_ds = random_split(ds, [train_len, val_len])
                final_loaders[key] = DataLoader(train_ds, batch_size=config.batch_size, shuffle=True, collate_fn=collator)
                val_datasets.append(val_ds)
            else:
                # ãƒ‡ãƒ¼ã‚¿ãŒå°‘ãªã™ãã‚‹å ´åˆã¯å…¨éƒ¨Train
                final_loaders[key] = DataLoader(ds, batch_size=config.batch_size, shuffle=True, collate_fn=collator)

    # é›†ã‚ãŸãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆã—ã¦1ã¤ã®Loaderã«ã™ã‚‹
    if val_datasets:
        combined_val_ds = ConcatDataset(val_datasets)
        final_loaders["val"] = DataLoader(combined_val_ds, batch_size=config.batch_size, shuffle=False, collate_fn=collator)
        logger.info(f"ğŸ“Š Validation Dataset Created: {len(combined_val_ds)} samples")
    else:
        logger.warning("âš ï¸ No validation dataset created (data might be too small).")

    return final_loaders